{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Building a GPT\n",
        "\n",
        "Companion notebook to the [Zero To Hero](https://karpathy.ai/zero-to-hero.html) video on GPT."
      ],
      "metadata": {
        "id": "wJpXpmjEYC_T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load data"
      ],
      "metadata": {
        "id": "yidXi2hohLnd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5hjCcLDr2WC",
        "outputId": "0333a193-f945-45c7-8bcb-756175ca2d60"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-15 04:10:14--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-11-15 04:10:14 (19.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# We always start with a dataset to train on. Let's download the tiny shakespeare dataset\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "O6medjfRsLD9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"length of dataset in characters: \", len(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xWI_VyAsN8F",
        "outputId": "992b0682-eb8f-4cd1-ba14-d0a19b9feb1e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "length of dataset in characters:  1115394\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's look at the first 1000 characters\n",
        "print(text[:1000])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c5V0FvqseE0",
        "outputId": "6082161a-af61-43c3-8818-f646474e1136"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us kill him, and we'll have corn at our own price.\n",
            "Is't a verdict?\n",
            "\n",
            "All:\n",
            "No more talking on't; let it be done: away, away!\n",
            "\n",
            "Second Citizen:\n",
            "One word, good citizens.\n",
            "\n",
            "First Citizen:\n",
            "We are accounted poor citizens, the patricians good.\n",
            "What authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they think we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to particularise their abundance; our\n",
            "sufferance is a gain to them Let us revenge this with\n",
            "our pikes, ere we become rakes: for the gods know I\n",
            "speak this in hunger for bread, not in thirst for revenge.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(''.join(chars))\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e-Rbyr8sfM8",
        "outputId": "96db0721-d2cf-459a-a454-93e0fa021d02"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenizer, Encoder, Decoder (simplified)\n",
        "\n",
        "### Tokenizer\n",
        "A tokenizer is a tool that splits a given input (like a sentence) into smaller units called tokens. These tokens can be words, characters, or subwords. In the provided code, the tokenizer implicitly treats each character as a token. This is evident from the fact that the mappings `stoi` (string to integer) and `itos` (integer to string) are created based on individual characters (`chars`) in the input.\n",
        "\n",
        "### Encoder\n",
        "An encoder converts the tokens into a format that a machine learning model can understand, typically numerical. In this code, the `encode` function is the encoder. It takes a string as input and converts each character in the string to its corresponding integer based on the `stoi` mapping. This process transforms the textual data into a list of integers, which is a format suitable for computational models.\n",
        "\n",
        "### Decoder\n",
        "A decoder performs the reverse operation of the encoder. It converts the machine-readable format (like integers) back into a human-readable format (like text). In the code, the `decode` function is the decoder. It takes a list of integers (the encoded format) and converts each integer back to its corresponding character using the `itos` mapping. Then, it joins these characters to form a string, reconstructing the original text or a translated version.\n",
        "\n",
        "### Overall Workflow\n",
        "The overall process in the code demonstrates a basic NLP pipeline:\n",
        "1. **Tokenization**: The input string is implicitly tokenized into characters.\n",
        "2. **Encoding**: Each character (token) is converted into an integer.\n",
        "3. **Processing**: While not explicitly shown in the code, this step would typically involve some form of processing or transformation, like passing the encoded data through a machine learning model.\n",
        "4. **Decoding**: The processed data is converted back into a human-readable format, reconstructing the original message or translating it into another form.\n",
        "\n",
        "This kind of tokenizer, encoder, and decoder framework is fundamental in many areas of NLP and is particularly crucial in models like machine translation, text summarization, and language generation models."
      ],
      "metadata": {
        "id": "oZKZxGTrg-bG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "print(encode(\"hii there\"))\n",
        "print(decode([46, 47, 47, 1, 58, 46, 43, 56, 43]))\n",
        "print(decode(encode(\"hii there\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yw1LKNCgwjj1",
        "outputId": "44075e82-5a5b-429a-ff80-0c528daad029"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[46, 47, 47, 1, 58, 46, 43, 56, 43]\n",
            "hii there\n",
            "hii there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# let's now encode the entire text dataset and store it into a torch.Tensor\n",
        "import torch # we use PyTorch: https://pytorch.org\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "print(data.shape, data.dtype)\n",
        "print(data[:1000]) # the 1000 characters we looked at earier will to the GPT look like this"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YJb0OXPwzvqg",
        "outputId": "5d9ad6d2-07b7-4865-e695-e94136d06479"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1115394]) torch.int64\n",
            "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
            "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
            "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56,  1, 51, 43,  1,\n",
            "        57, 54, 43, 39, 49,  8,  0,  0, 13, 50, 50, 10,  0, 31, 54, 43, 39, 49,\n",
            "         6,  1, 57, 54, 43, 39, 49,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47,\n",
            "        58, 47, 64, 43, 52, 10,  0, 37, 53, 59,  1, 39, 56, 43,  1, 39, 50, 50,\n",
            "         1, 56, 43, 57, 53, 50, 60, 43, 42,  1, 56, 39, 58, 46, 43, 56,  1, 58,\n",
            "        53,  1, 42, 47, 43,  1, 58, 46, 39, 52,  1, 58, 53,  1, 44, 39, 51, 47,\n",
            "        57, 46, 12,  0,  0, 13, 50, 50, 10,  0, 30, 43, 57, 53, 50, 60, 43, 42,\n",
            "         8,  1, 56, 43, 57, 53, 50, 60, 43, 42,  8,  0,  0, 18, 47, 56, 57, 58,\n",
            "         1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 18, 47, 56, 57, 58,  6,  1, 63,\n",
            "        53, 59,  1, 49, 52, 53, 61,  1, 15, 39, 47, 59, 57,  1, 25, 39, 56, 41,\n",
            "        47, 59, 57,  1, 47, 57,  1, 41, 46, 47, 43, 44,  1, 43, 52, 43, 51, 63,\n",
            "         1, 58, 53,  1, 58, 46, 43,  1, 54, 43, 53, 54, 50, 43,  8,  0,  0, 13,\n",
            "        50, 50, 10,  0, 35, 43,  1, 49, 52, 53, 61,  5, 58,  6,  1, 61, 43,  1,\n",
            "        49, 52, 53, 61,  5, 58,  8,  0,  0, 18, 47, 56, 57, 58,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 24, 43, 58,  1, 59, 57,  1, 49, 47, 50, 50,  1,\n",
            "        46, 47, 51,  6,  1, 39, 52, 42,  1, 61, 43,  5, 50, 50,  1, 46, 39, 60,\n",
            "        43,  1, 41, 53, 56, 52,  1, 39, 58,  1, 53, 59, 56,  1, 53, 61, 52,  1,\n",
            "        54, 56, 47, 41, 43,  8,  0, 21, 57,  5, 58,  1, 39,  1, 60, 43, 56, 42,\n",
            "        47, 41, 58, 12,  0,  0, 13, 50, 50, 10,  0, 26, 53,  1, 51, 53, 56, 43,\n",
            "         1, 58, 39, 50, 49, 47, 52, 45,  1, 53, 52,  5, 58, 11,  1, 50, 43, 58,\n",
            "         1, 47, 58,  1, 40, 43,  1, 42, 53, 52, 43, 10,  1, 39, 61, 39, 63,  6,\n",
            "         1, 39, 61, 39, 63,  2,  0,  0, 31, 43, 41, 53, 52, 42,  1, 15, 47, 58,\n",
            "        47, 64, 43, 52, 10,  0, 27, 52, 43,  1, 61, 53, 56, 42,  6,  1, 45, 53,\n",
            "        53, 42,  1, 41, 47, 58, 47, 64, 43, 52, 57,  8,  0,  0, 18, 47, 56, 57,\n",
            "        58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 35, 43,  1, 39, 56, 43,  1,\n",
            "        39, 41, 41, 53, 59, 52, 58, 43, 42,  1, 54, 53, 53, 56,  1, 41, 47, 58,\n",
            "        47, 64, 43, 52, 57,  6,  1, 58, 46, 43,  1, 54, 39, 58, 56, 47, 41, 47,\n",
            "        39, 52, 57,  1, 45, 53, 53, 42,  8,  0, 35, 46, 39, 58,  1, 39, 59, 58,\n",
            "        46, 53, 56, 47, 58, 63,  1, 57, 59, 56, 44, 43, 47, 58, 57,  1, 53, 52,\n",
            "         1, 61, 53, 59, 50, 42,  1, 56, 43, 50, 47, 43, 60, 43,  1, 59, 57, 10,\n",
            "         1, 47, 44,  1, 58, 46, 43, 63,  0, 61, 53, 59, 50, 42,  1, 63, 47, 43,\n",
            "        50, 42,  1, 59, 57,  1, 40, 59, 58,  1, 58, 46, 43,  1, 57, 59, 54, 43,\n",
            "        56, 44, 50, 59, 47, 58, 63,  6,  1, 61, 46, 47, 50, 43,  1, 47, 58,  1,\n",
            "        61, 43, 56, 43,  0, 61, 46, 53, 50, 43, 57, 53, 51, 43,  6,  1, 61, 43,\n",
            "         1, 51, 47, 45, 46, 58,  1, 45, 59, 43, 57, 57,  1, 58, 46, 43, 63,  1,\n",
            "        56, 43, 50, 47, 43, 60, 43, 42,  1, 59, 57,  1, 46, 59, 51, 39, 52, 43,\n",
            "        50, 63, 11,  0, 40, 59, 58,  1, 58, 46, 43, 63,  1, 58, 46, 47, 52, 49,\n",
            "         1, 61, 43,  1, 39, 56, 43,  1, 58, 53, 53,  1, 42, 43, 39, 56, 10,  1,\n",
            "        58, 46, 43,  1, 50, 43, 39, 52, 52, 43, 57, 57,  1, 58, 46, 39, 58,  0,\n",
            "        39, 44, 44, 50, 47, 41, 58, 57,  1, 59, 57,  6,  1, 58, 46, 43,  1, 53,\n",
            "        40, 48, 43, 41, 58,  1, 53, 44,  1, 53, 59, 56,  1, 51, 47, 57, 43, 56,\n",
            "        63,  6,  1, 47, 57,  1, 39, 57,  1, 39, 52,  0, 47, 52, 60, 43, 52, 58,\n",
            "        53, 56, 63,  1, 58, 53,  1, 54, 39, 56, 58, 47, 41, 59, 50, 39, 56, 47,\n",
            "        57, 43,  1, 58, 46, 43, 47, 56,  1, 39, 40, 59, 52, 42, 39, 52, 41, 43,\n",
            "        11,  1, 53, 59, 56,  0, 57, 59, 44, 44, 43, 56, 39, 52, 41, 43,  1, 47,\n",
            "        57,  1, 39,  1, 45, 39, 47, 52,  1, 58, 53,  1, 58, 46, 43, 51,  1, 24,\n",
            "        43, 58,  1, 59, 57,  1, 56, 43, 60, 43, 52, 45, 43,  1, 58, 46, 47, 57,\n",
            "         1, 61, 47, 58, 46,  0, 53, 59, 56,  1, 54, 47, 49, 43, 57,  6,  1, 43,\n",
            "        56, 43,  1, 61, 43,  1, 40, 43, 41, 53, 51, 43,  1, 56, 39, 49, 43, 57,\n",
            "        10,  1, 44, 53, 56,  1, 58, 46, 43,  1, 45, 53, 42, 57,  1, 49, 52, 53,\n",
            "        61,  1, 21,  0, 57, 54, 43, 39, 49,  1, 58, 46, 47, 57,  1, 47, 52,  1,\n",
            "        46, 59, 52, 45, 43, 56,  1, 44, 53, 56,  1, 40, 56, 43, 39, 42,  6,  1,\n",
            "        52, 53, 58,  1, 47, 52,  1, 58, 46, 47, 56, 57, 58,  1, 44, 53, 56,  1,\n",
            "        56, 43, 60, 43, 52, 45, 43,  8,  0,  0])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Train, Valid, Test\n",
        "\n",
        "\n",
        "**Splitting the Data**:\n",
        "   - `train_data = data[:n]`: This line creates the training dataset (`train_data`) by taking the first `n` elements from the `data`. Since `n` is 90% of the total data, this means `train_data` will have 90% of the entries from the original dataset.\n",
        "   - `val_data = data[n:]`: This line creates the validation dataset (`val_data`) by taking the remaining data points in `data` from the `n`th index to the end. This subset will be the remaining 10% of the data not included in the training set.\n",
        "\n",
        "### Importance of Training, Validation, and Testing Sets\n",
        "\n",
        "#### Training Set\n",
        "- **Purpose**: The training set is used to train the machine learning model. It is the primary dataset on which the model learns to make predictions or classifications.\n",
        "- **Importance**: Without a training set, the model would have no examples to learn from, similar to how students need lectures or textbooks to learn a subject.\n",
        "\n",
        "#### Validation Set\n",
        "- **Purpose**: The validation set is used to evaluate the model during the training process. It helps in tuning the model's parameters and provides a check against overfitting.\n",
        "- **Overfitting**: Overfitting is when a model performs well on the training data but poorly on new, unseen data. It's like a student who memorizes facts for a test but doesn't understand the concepts well enough to apply them in different situations.\n",
        "- **Importance**: The validation set acts like a practice exam. It helps understand how well the model is learning and generalizing beyond the training data.\n",
        "\n",
        "#### Testing Set\n",
        "- **Purpose**: The testing set is used to evaluate the model's performance after the training is complete. It's a final check to see how well the model will perform on entirely new data.\n",
        "- **Importance**: The test set is like a final exam taken after all the studies and revisions are done. It gives an unbiased evaluation of the model's performance in real-world scenarios.\n",
        "\n",
        "#### Simplified Explanation for Students\n",
        "Think of the whole machine learning process like preparing for a big exam:\n",
        "- **Training Set**: This is your study material, like textbooks and notes, which you use to learn and understand the topics.\n",
        "- **Validation Set**: These are like practice tests you take while studying. They help you see what you've learned well and what you need to study more. They also stop you from just memorizing the textbook (preventing overfitting).\n",
        "- **Testing Set**: This is the final exam, taken after all your studies and practice tests. It shows how well you've learned the material and can apply it to new questions you've never seen before."
      ],
      "metadata": {
        "id": "Cq3NSp5Phbor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's now split up the data into train and validation sets\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "f_WIXqxz0lU5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code snippet is designed for batching in a decoder-only model, commonly used in tasks like language modeling where the goal is to predict the next token in a sequence. Let's first understand the code and then delve into the concept of a decoder-only model.\n",
        "\n",
        "### Understanding the Code\n",
        "\n",
        "1. **Setting the Seed and Parameters**:\n",
        "   - `torch.manual_seed(1337)` sets a fixed seed for random number generation in PyTorch, ensuring reproducibility of results.\n",
        "   - `batch_size` determines how many sequences are processed in parallel. Here, it's set to 4.\n",
        "   - `block_size` is the maximum context length for predictions, set to 8.\n",
        "\n",
        "2. **Batch Generation (`get_batch` function)**:\n",
        "   - The function selects either `train_data` or `val_data` based on the `split` argument ('train' or 'val').\n",
        "   - It generates indices (`ix`) for selecting random starting points in the data.\n",
        "   - Inputs (`x`) and targets (`y`) are created based on these indices. Each input sequence in `x` is a sequence of `block_size` tokens from `data`, and each target sequence in `y` is the subsequent token in the sequence, used as the prediction target.\n",
        "\n",
        "3. **Printing Batch Details**:\n",
        "   - The shapes and contents of the input (`xb`) and target (`yb`) batches are printed.\n",
        "   - The loop at the end iteratively prints each context and its corresponding target from the batch. This illustrates how the model should predict the next token based on the given context.\n",
        "\n",
        "### Decoder-Only Model Explanation\n",
        "\n",
        "In machine learning, particularly in natural language processing, a decoder-only model is structured to generate text or predict the next item in a sequence. Here’s why it’s useful and how it works:\n",
        "\n",
        "1. **Purpose**: A decoder-only model is designed for tasks where the output is a continuation of the input, like text generation, where you predict the next word in a sentence.\n",
        "\n",
        "2. **Functioning**:\n",
        "   - **Context Understanding**: The model takes a sequence of tokens (words, characters, etc.) as input and understands the context.\n",
        "   - **Next Token Prediction**: Based on this context, it predicts the next token in the sequence.\n",
        "\n",
        "3. **Applications**: This model architecture is prominent in applications like language models (GPT series, for example), where the model generates text, completes sentences, or even writes code based on the given prompt.\n",
        "\n",
        "4. **Training Process**: During training, the model learns by looking at a part of a text sequence and predicting the next part. This is what the provided code is preparing the data for. By repeatedly practicing this task, the model learns patterns in language and can generate coherent and contextually relevant text.\n",
        "\n",
        "In simple terms, a decoder-only model in language tasks is like a skilled storyteller. You give it the beginning of a story (the context), and it learns to continue the story in a way that makes sense and is engaging, learning better storytelling techniques as it practices on more and more stories (training data)."
      ],
      "metadata": {
        "id": "yuWIiajHpmBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TD5Bj8Y6IAD4",
        "outputId": "1a382b4e-95e6-4894-bd85-232beec870e2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1]\n",
        "    target = y[t]\n",
        "    print(f\"when input is {context} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9HXDe8vGJCEn",
        "outputId": "387fe8fc-a425-45a1-ffca-ecba23d44401"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "when input is tensor([18]) the target: 47\n",
            "when input is tensor([18, 47]) the target: 56\n",
            "when input is tensor([18, 47, 56]) the target: 57\n",
            "when input is tensor([18, 47, 56, 57]) the target: 58\n",
            "when input is tensor([18, 47, 56, 57, 58]) the target: 1\n",
            "when input is tensor([18, 47, 56, 57, 58,  1]) the target: 15\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15]) the target: 47\n",
            "when input is tensor([18, 47, 56, 57, 58,  1, 15, 47]) the target: 58\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "batch_size = 4 # how many independent sequences will we process in parallel?\n",
        "block_size = 8 # what is the maximum context length for predictions?\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)\n",
        "\n",
        "print('----')\n",
        "\n",
        "for b in range(batch_size): # batch dimension\n",
        "    for t in range(block_size): # time dimension\n",
        "        context = xb[b, :t+1]\n",
        "        target = yb[b,t]\n",
        "        print(f\"when input is {context.tolist()} the target: {target}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3k1Czf7LuA9",
        "outputId": "3b76ce36-3888-4051-bcdb-50f21831887d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[43, 58,  5, 57,  1, 46, 43, 39],\n",
            "        [53, 56,  1, 58, 46, 39, 58,  1],\n",
            "        [58,  1, 58, 46, 39, 58,  1, 46],\n",
            "        [17, 27, 10,  0, 21,  1, 54, 39]])\n",
            "----\n",
            "when input is [24] the target: 43\n",
            "when input is [24, 43] the target: 58\n",
            "when input is [24, 43, 58] the target: 5\n",
            "when input is [24, 43, 58, 5] the target: 57\n",
            "when input is [24, 43, 58, 5, 57] the target: 1\n",
            "when input is [24, 43, 58, 5, 57, 1] the target: 46\n",
            "when input is [24, 43, 58, 5, 57, 1, 46] the target: 43\n",
            "when input is [24, 43, 58, 5, 57, 1, 46, 43] the target: 39\n",
            "when input is [44] the target: 53\n",
            "when input is [44, 53] the target: 56\n",
            "when input is [44, 53, 56] the target: 1\n",
            "when input is [44, 53, 56, 1] the target: 58\n",
            "when input is [44, 53, 56, 1, 58] the target: 46\n",
            "when input is [44, 53, 56, 1, 58, 46] the target: 39\n",
            "when input is [44, 53, 56, 1, 58, 46, 39] the target: 58\n",
            "when input is [44, 53, 56, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52] the target: 58\n",
            "when input is [52, 58] the target: 1\n",
            "when input is [52, 58, 1] the target: 58\n",
            "when input is [52, 58, 1, 58] the target: 46\n",
            "when input is [52, 58, 1, 58, 46] the target: 39\n",
            "when input is [52, 58, 1, 58, 46, 39] the target: 58\n",
            "when input is [52, 58, 1, 58, 46, 39, 58] the target: 1\n",
            "when input is [52, 58, 1, 58, 46, 39, 58, 1] the target: 46\n",
            "when input is [25] the target: 17\n",
            "when input is [25, 17] the target: 27\n",
            "when input is [25, 17, 27] the target: 10\n",
            "when input is [25, 17, 27, 10] the target: 0\n",
            "when input is [25, 17, 27, 10, 0] the target: 21\n",
            "when input is [25, 17, 27, 10, 0, 21] the target: 1\n",
            "when input is [25, 17, 27, 10, 0, 21, 1] the target: 54\n",
            "when input is [25, 17, 27, 10, 0, 21, 1, 54] the target: 39\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(xb) # our input to the transformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qpyyAeIzQjlO",
        "outputId": "69d6722a-4687-42e6-9535-665671330b1b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[24, 43, 58,  5, 57,  1, 46, 43],\n",
            "        [44, 53, 56,  1, 58, 46, 39, 58],\n",
            "        [52, 58,  1, 58, 46, 39, 58,  1],\n",
            "        [25, 17, 27, 10,  0, 21,  1, 54]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines a basic bigram language model using PyTorch, a popular deep learning library. The model predicts the next word in a sequence based on the current word. Let's break down the code and explain its components, including the cross-entropy loss and the shapes of the vectors involved.\n",
        "\n",
        "### Cross-Entropy Loss\n",
        "Cross-entropy loss is commonly used in classification tasks. It measures the difference between two probability distributions: the true distribution (from the target labels) and the predicted distribution (from the model's output). In this context, it's used to measure how well the model's predictions match the actual next words (targets).\n",
        "\n",
        "### Shape of Vectors\n",
        "- **`logits` Shape**: The shape of `logits` is (B, T, C), where B is the batch size, T is the sequence length (number of tokens in each sequence), and C is the number of classes (same as the vocabulary size).\n",
        "- **`targets` Shape**: The `targets` tensor is reshaped to (B*T) to align with the flattened `logits` for loss calculation. This is necessary because cross-entropy loss in PyTorch expects inputs of shape (N, C) where N is the number of samples (here, B*T) and C is the number of classes.\n",
        "\n",
        "### Overall Functionality\n",
        "This model takes sequences of token indices as input and predicts the next token in each sequence. It uses an embedding layer to map tokens to vectors and calculates the cross-entropy loss to measure the accuracy of its predictions. The `generate` function allows the model to create new text sequences based on an initial input."
      ],
      "metadata": {
        "id": "25FkpOq8t-NR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(1337)  # Setting a random seed for reproducibility\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    # A language model based on bigrams (pairs of consecutive words)\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # Embedding layer: maps each token to a vector with the same size as the vocabulary\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # Forward pass of the model\n",
        "        # idx: input tensor containing indices of the current tokens\n",
        "\n",
        "        logits = self.token_embedding_table(idx)  # Convert indices to logits\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None  # No loss calculation if targets are not provided\n",
        "        else:\n",
        "            B, T, C = logits.shape  # Batch size (B), sequence length (T), and number of classes (C)\n",
        "            logits = logits.view(B*T, C)  # Reshape logits for cross-entropy calculation\n",
        "            targets = targets.view(B*T)  # Flatten the targets\n",
        "            loss = F.cross_entropy(logits, targets)  # Calculate the cross-entropy loss\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # Function to generate text from the model\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits, loss = self(idx)\n",
        "            logits = logits[:, -1, :]  # Focus on the last set of logits\n",
        "            probs = F.softmax(logits, dim=-1)  # Convert logits to probabilities\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # Sample the next token\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # Append the next token to the sequence\n",
        "\n",
        "        return idx\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)  # Forward pass with inputs xb and targets yb\n",
        "print(logits.shape)  # Print the shape of the logits tensor\n",
        "print(loss)  # Print the loss\n",
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))\n",
        "# Generate text starting with an initial index of zero and create 100 new tokens\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nql_1ER53oCf",
        "outputId": "8301045b-1fed-4fe3-a752-f4ea611365f7"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([32, 65])\n",
            "tensor(4.8786, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "Sr?qP-QWktXoL&jLDJgOLVz'RIoDqHdhsV&vLLxatjscMpwLERSPyao.qfzs$Ys$zF-w,;eEkzxjgCKFChs!iWW.ObzDnxA Ms$3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)"
      ],
      "metadata": {
        "id": "eTyJ8qAaDdiF"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "for steps in range(1000): # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs4kI8YdEkQj",
        "outputId": "55d96175-fe92-45af-bb76-824751d768b5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.7218432426452637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(m.generate(idx = torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcVIDWAZEtjN",
        "outputId": "ca457385-831e-4750-f9bd-20971fca243a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "olylvLLko'TMyatyIoconxad.?-tNSqYPsx&bF.oiR;BD$dZBMZv'K f bRSmIKptRPly:AUC&$zLK,qUEy&Ay;ZxjKVhmrdagC-bTop-QJe.H?x\n",
            "JGF&pwst-P sti.hlEsu;w:w a BG:tLhMk,epdhlay'sVzLq--ERwXUzDnq-bn czXxxI&V&Pynnl,s,Ioto!uvixwC-IJXElrgm C-.bcoCPJ\n",
            "IMphsevhO AL!-K:AIkpre,\n",
            "rPHEJUzV;P?uN3b?ohoRiBUENoV3B&jumNL;Aik,\n",
            "xf -IEKROn JSyYWW?n 'ay;:weO'AqVzPyoiBL? seAX3Dot,iy.xyIcf r!!ul-Koi:x pZrAQly'v'a;vEzN\n",
            "BwowKo'MBqF$PPFb\n",
            "CjYX3beT,lZ qdda!wfgmJP\n",
            "DUfNXmnQU mvcv?nlnQF$JUAAywNocd  bGSPyAlprNeQnq-GRSVUP.Ja!IBoDqfI&xJM AXEHV&DKvRS\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The mathematical trick in self-attention"
      ],
      "metadata": {
        "id": "XinV8nmAnmKN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# toy example illustrating how matrix multiplication can be used for a \"weighted aggregation\"\n",
        "torch.manual_seed(42)\n",
        "a = torch.tril(torch.ones(3, 3))\n",
        "a = a / torch.sum(a, 1, keepdim=True)\n",
        "b = torch.randint(0,10,(3,2)).float()\n",
        "c = a @ b\n",
        "print('a=')\n",
        "print(a)\n",
        "print('--')\n",
        "print('b=')\n",
        "print(b)\n",
        "print('--')\n",
        "print('c=')\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tukiH-NbRBhA",
        "outputId": "bf879b15-8615-402f-89fe-b393eaf43f98"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a=\n",
            "tensor([[1.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333]])\n",
            "--\n",
            "b=\n",
            "tensor([[2., 7.],\n",
            "        [6., 4.],\n",
            "        [6., 5.]])\n",
            "--\n",
            "c=\n",
            "tensor([[2.0000, 7.0000],\n",
            "        [4.0000, 5.5000],\n",
            "        [4.6667, 5.3333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hs_E24uRE8kr",
        "outputId": "438576ee-0ae1-45e3-e617-86c76091604b"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.67 ms, sys: 1.17 ms, total: 2.85 ms\n",
            "Wall time: 7.42 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# We want x[b,t] = mean_{i<=t} x[b,i]\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n"
      ],
      "metadata": {
        "id": "86NuXX0fn7ps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6af4c6-085d-40c1-c506-6e548daa8175"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.67 ms, sys: 869 µs, total: 2.54 ms\n",
            "Wall time: 2.33 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "torch.allclose(xbow, xbow2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhdOAd6-wXkZ",
        "outputId": "7c976638-6df4-4190-8d0c-6b35e4ec9c83"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.36 ms, sys: 0 ns, total: 1.36 ms\n",
            "Wall time: 4.74 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "torch.allclose(xbow, xbow3)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOURrfG-ysoL",
        "outputId": "8f8017f6-e781-4263-9e76-3c877cbef0e6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 1.55 ms, sys: 0 ns, total: 1.55 ms\n",
            "Wall time: 1.56 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# version 4: self-attention!\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 4,8,32 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "\n",
        "# let's see a single Head perform self-attention\n",
        "head_size = 16\n",
        "key = nn.Linear(C, head_size, bias=False)\n",
        "query = nn.Linear(C, head_size, bias=False)\n",
        "value = nn.Linear(C, head_size, bias=False)\n",
        "k = key(x)   # (B, T, 16)\n",
        "q = query(x) # (B, T, 16)\n",
        "wei =  q @ k.transpose(-2, -1) # (B, T, 16) @ (B, 16, T) ---> (B, T, T)\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "#wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "#out = wei @ x\n",
        "\n",
        "out.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EDarxEWIRMKq",
        "outputId": "e8fd3086-7f3d-407e-81ac-50ec8a77b512"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CPU times: user 2.2 ms, sys: 921 µs, total: 3.12 ms\n",
            "Wall time: 5.91 ms\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([4, 8, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vT1hdtzXCjgL",
        "outputId": "bae6a36f-458e-4ccd-b100-257774e7dc83"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.1574, 0.8426, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.2088, 0.1646, 0.6266, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.5792, 0.1187, 0.1889, 0.1131, 0.0000, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0294, 0.1052, 0.0469, 0.0276, 0.7909, 0.0000, 0.0000, 0.0000],\n",
              "        [0.0176, 0.2689, 0.0215, 0.0089, 0.6812, 0.0019, 0.0000, 0.0000],\n",
              "        [0.1691, 0.4066, 0.0438, 0.0416, 0.1048, 0.2012, 0.0329, 0.0000],\n",
              "        [0.0210, 0.0843, 0.0555, 0.2297, 0.0573, 0.0709, 0.2423, 0.2391]],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More details - Notes\n",
        "Notes:\n",
        "- Attention is a **communication mechanism**. Can be seen as nodes in a directed graph looking at each other and aggregating information with a weighted sum from all nodes that point to them, with data-dependent weights.\n",
        "- There is no notion of space. Attention simply acts over a set of vectors. This is why we need to positionally encode tokens.\n",
        "- Each example across batch dimension is of course processed completely independently and never \"talk\" to each other\n",
        "- In an \"encoder\" attention block just delete the single line that does masking with `tril`, allowing all tokens to communicate. This block here is called a \"decoder\" attention block because it has triangular masking, and is usually used in autoregressive settings, like language modeling.\n",
        "- \"self-attention\" just means that the keys and values are produced from the same source as queries. In \"cross-attention\", the queries still get produced from x, but the keys and values come from some other, external source (e.g. an encoder module)\n",
        "- \"Scaled\" attention additional divides `wei` by 1/sqrt(head_size). This makes it so when input Q,K are unit variance, wei will be unit variance too and Softmax will stay diffuse and not saturate too much. Illustration below\n",
        "\n",
        "\n",
        "-------------------\n",
        "\n",
        "The notes you've provided summarize key concepts related to attention mechanisms in neural networks, particularly in the context of models like transformers. Let's delve into these concepts with more detail and then provide a simplified explanation.\n",
        "\n",
        "### Detailed Explanation\n",
        "\n",
        "1. **Attention as a Communication Mechanism**:\n",
        "   - In neural networks, attention allows different parts of the input data to 'communicate' with each other. Imagine attention as nodes in a graph where each node can 'look' at other nodes and gather information from them. The information aggregation is done through a weighted sum, where the weights are determined based on the relevance of other nodes' information to the current node.\n",
        "\n",
        "2. **No Notion of Space and Need for Positional Encoding**:\n",
        "   - Attention mechanisms operate on sets of vectors without inherent ordering or positioning. This means they don't naturally understand the order of words in a sentence or pixels in an image.\n",
        "   - To address this, we use positional encoding. It adds extra information to each vector to indicate its position (or order) in the sequence, allowing the model to understand sequence order or spatial relationships.\n",
        "\n",
        "3. **Independent Processing Across Batch Dimension**:\n",
        "   - In batch processing, multiple examples (like sentences or images) are processed simultaneously for efficiency. However, each example in a batch is processed independently. That means information from one example in the batch does not influence the processing of another.\n",
        "\n",
        "4. **Encoder vs. Decoder Attention Blocks**:\n",
        "   - In attention-based models, there are typically two types of blocks: encoder and decoder blocks.\n",
        "   - Encoder blocks allow each token (like a word in a sentence) to attend to all other tokens. There is no restriction on what each token can 'see'.\n",
        "   - Decoder blocks, on the other hand, use triangular masking (with `tril`). This masking prevents a token from attending to future tokens, which is crucial in autoregressive tasks (like generating text, where the future words are not yet known).\n",
        "\n",
        "5. **Self-Attention and Cross-Attention**:\n",
        "   - Self-attention means the keys, queries, and values in the attention mechanism all come from the same input source. Essentially, the input data is interacting with itself, finding relationships within.\n",
        "   - Cross-attention, however, uses queries from one source (like the current input in a decoder) and keys and values from another source (like the output of an encoder). This is common in tasks like machine translation, where the model needs to relate two different sequences (like sentences in two languages).\n",
        "\n",
        "6. **Scaled Attention**:\n",
        "   - In scaled attention, the attention weights are scaled down by a factor of 1/sqrt(head_size) (head size is a dimension of the key/query vectors). This scaling helps in stabilizing the learning process.\n",
        "   - It ensures that the attention weights don't become too large, preventing the softmax function from becoming too 'sharp'. A sharp softmax would mean the model pays attention to very few tokens excessively and ignores the rest, which is not desirable for learning nuanced relationships in the data.\n",
        "\n",
        "### Simplified Explanation\n",
        "\n",
        "Imagine you're in a room full of people, and you're trying to figure out the most relevant conversations to pay attention to:\n",
        "\n",
        "- **Attention Mechanism**: It's like you have a special ability to listen to multiple conversations at once and decide which ones are most important based on what's being said.\n",
        "- **Positional Encoding**: Since people talk in sequences (one word after the other), you need to understand the order of words in each conversation. Positional encoding is like your ability to remember the sequence of words in each conversation.\n",
        "- **Independent Processing**: Even if there are multiple groups in the room, what happens in one group doesn't affect your understanding of another group.\n",
        "- **Encoder vs. Decoder**: If you're just listening (encoder), you can hear all parts of a conversation. But if you're also talking (decoder), you can't talk about parts of the conversation that haven't happened yet.\n",
        "- **Self-Attention vs. Cross-Attention**: Self-attention is like understanding a conversation by relating different parts of the same conversation. Cross-attention is like using information from one conversation to understand another.\n",
        "- **Scaled Attention**: This is akin to not focusing too much on a loud voice in the room (scaling down), so you can still understand the general noise level and not miss out on other important conversations."
      ],
      "metadata": {
        "id": "M5CvobiQ0pLr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Defining the BigramLanguageModel class which inherits from nn.Module\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, device):\n",
        "        super().__init__()\n",
        "        # Initialize embeddings and network layers\n",
        "\n",
        "        # Token embedding table, converting token indices into embeddings\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "\n",
        "        # Positional embedding table, encoding the position of each token in a sequence\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "\n",
        "        # A sequence of transformer blocks for processing the embeddings\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "\n",
        "        # Layer normalization applied to the output of the last block\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "\n",
        "        # Linear transformation to project the output back to the vocabulary space\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # Forward pass of the model\n",
        "\n",
        "        # Extracting batch size (B) and sequence length (T) from input indices\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # Getting token embeddings for the input indices\n",
        "        tok_emb = self.token_embedding_table(idx)  # Shape: (B, T, C)\n",
        "\n",
        "        # Generating positional embeddings for each position in the sequence\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device))  # Shape: (T, C)\n",
        "\n",
        "        # Adding token embeddings and positional embeddings\n",
        "        x = tok_emb + pos_emb  # Shape: (B, T, C)\n",
        "\n",
        "        # Passing the combined embeddings through the transformer blocks\n",
        "        x = self.blocks(x)  # Shape: (B, T, C)\n",
        "\n",
        "        # Applying layer normalization\n",
        "        x = self.ln_f(x)  # Shape: (B, T, C)\n",
        "\n",
        "        # Projecting the output to vocabulary size to get logits\n",
        "        logits = self.lm_head(x)  # Shape: (B, T, vocab_size)\n",
        "\n",
        "        if targets is not None:\n",
        "            # Compute loss if targets are provided\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B * T, C)\n",
        "            targets = targets.view(B * T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "        else:\n",
        "            # No loss computation if targets are not provided\n",
        "            loss = None\n",
        "\n",
        "        return logits, loss\n"
      ],
      "metadata": {
        "id": "DNVPKev4FR9z"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k = torch.randn(B,T,head_size)\n",
        "q = torch.randn(B,T,head_size)\n",
        "wei = q @ k.transpose(-2, -1) * head_size**-0.5"
      ],
      "metadata": {
        "id": "4SNbLq5z3oBw"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "k.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl6I9n9IRTSo",
        "outputId": "65a62a63-26f7-4632-e6b6-701f1d4120e5"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0449)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "q.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1tQx7oeRvtc",
        "outputId": "98dbae5a-4fe9-4239-83dd-a428cbfc54e6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0700)"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "wei.var()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLb_odHU3iKM",
        "outputId": "901dbacd-9507-4a2e-a685-ba1e564a41b6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.0918)"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5]), dim=-1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JB82yzt44REI",
        "outputId": "12b1db5e-5eaf-4e80-b5f0-6004b12ed4fa"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1925, 0.1426, 0.2351, 0.1426, 0.2872])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.softmax(torch.tensor([0.1, -0.2, 0.3, -0.2, 0.5])*8, dim=-1) # gets too peaky, converges to one-hot"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mpt8569BB9_f",
        "outputId": "2fadbce4-9402-41cb-ead7-be77da132231"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0326, 0.0030, 0.1615, 0.0030, 0.8000])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simplified Explanation\n",
        "\n",
        "Think of `LayerNorm1d` as a tool to make sure that the data being processed by a neural network is more standardized or normalized. Here's a simple analogy: Imagine you're a teacher grading a bunch of tests. Some tests are really tough, and some are very easy, so the scores are all over the place. To make them more comparable, you adjust the scores (normalization) so that each test has an average score of 0 and the spread of scores is consistent.\n",
        "\n",
        "- **Initialization (`__init__`)**: This is like setting up your grading scheme. You decide on the basic adjustments (parameters `gamma` and `beta`) you'll use to standardize scores. `gamma` is like a multiplier to adjust the spread of scores, and `beta` is like an add-on to adjust the average score.\n",
        "\n",
        "- **Applying Layer Normalization (`__call__`)**: Here, you take each test's scores, figure out how much they vary from the average (mean and variance), and then adjust them (normalize) using your grading scheme. This makes the scores on each test more comparable to the scores on other tests.\n",
        "\n",
        "- **Parameters (`parameters`)**: This method is like keeping a record of your grading scheme, so you know how you've adjusted the scores.\n",
        "\n",
        "- **Output**: After applying this layer normalization to a batch of data (like a bunch of tests), the result is a more standardized set of data that's easier for the neural network to work with, just like how normalized test scores are easier to compare and understand."
      ],
      "metadata": {
        "id": "ZBqwodeE0DnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNorm1d:  # Custom class for Layer Normalization\n",
        "\n",
        "    def __init__(self, dim, eps=1e-5, momentum=0.1):\n",
        "        # Constructor for initializing the layer\n",
        "        self.eps = eps  # A small number to prevent division by zero\n",
        "        self.gamma = torch.ones(dim)  # Scale parameter, initialized to ones\n",
        "        self.beta = torch.zeros(dim)  # Shift parameter, initialized to zeros\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # Method to apply layer normalization on input x\n",
        "        xmean = x.mean(1, keepdim=True)  # Calculate mean of each batch\n",
        "        xvar = x.var(1, keepdim=True)  # Calculate variance of each batch\n",
        "        xhat = (x - xmean) / torch.sqrt(xvar + self.eps)  # Normalize inputs\n",
        "        self.out = self.gamma * xhat + self.beta  # Apply scaling and shifting\n",
        "        return self.out\n",
        "\n",
        "    def parameters(self):\n",
        "        # Returns parameters of the layer (gamma and beta) for optimization\n",
        "        return [self.gamma, self.beta]\n",
        "\n",
        "# Set the seed for reproducibility\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# Create an instance of LayerNorm1d for 100-dimensional input\n",
        "module = LayerNorm1d(100)\n",
        "\n",
        "# Generate a batch of 32 random 100-dimensional vectors\n",
        "x = torch.randn(32, 100)\n",
        "\n",
        "# Apply layer normalization to the batch\n",
        "x = module(x)\n",
        "\n",
        "# Check the shape of the output\n",
        "x.shape\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Num7sX9CKOH",
        "outputId": "e44cf540-c14f-49fb-e5fc-e8e75dc237c9"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[:,0].mean(), x[:,0].std() # mean,std of one feature across all batch inputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "633T2cmnW1uk",
        "outputId": "50b16e59-e5c5-401f-9790-a6038e6e35ce"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(0.1469), tensor(0.8803))"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x[0,:].mean(), x[0,:].std() # mean,std of a single input from the batch, of its features"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LN9cK9BoXCYb",
        "outputId": "74d86686-9025-46c7-fe29-6a329e68bbd4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor(-9.5367e-09), tensor(1.0000))"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# French to English translation example:\n",
        "\n",
        "# <--------- ENCODE ------------------><--------------- DECODE ----------------->\n",
        "# les réseaux de neurones sont géniaux! <START> neural networks are awesome!<END>\n",
        "\n"
      ],
      "metadata": {
        "id": "dRJH6wM_XFfU"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Full finished code, for reference\n",
        "\n",
        "You may want to refer directly to the git repo instead though."
      ],
      "metadata": {
        "id": "ZcvKeBXoZFOY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Implements multi-head self-attention mechanism.\"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        # Initialize multiple attention heads\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "\n",
        "        # Linear projection layer\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "\n",
        "        # Dropout layer for regularization\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Concatenate the outputs from all attention heads\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "\n",
        "        # Apply linear projection and dropout\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    \"\"\"Implements a feed-forward neural network as part of the Transformer block.\"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        # Define a simple feed-forward network\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),  # Expand the input\n",
        "            nn.ReLU(),                      # ReLU activation function\n",
        "            nn.Linear(4 * n_embd, n_embd),  # Project back to original size\n",
        "            nn.Dropout(dropout),            # Dropout layer for regularization\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Pass the input through the feed-forward network\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoelkOrFY8bN",
        "outputId": "745bf707-394e-460f-f327-ee26ac60f075"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n",
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5091, val loss 2.5058\n",
            "step 300: train loss 2.4197, val loss 2.4336\n",
            "step 400: train loss 2.3501, val loss 2.3562\n",
            "step 500: train loss 2.2963, val loss 2.3125\n",
            "step 600: train loss 2.2407, val loss 2.2496\n",
            "step 700: train loss 2.2054, val loss 2.2187\n",
            "step 800: train loss 2.1633, val loss 2.1866\n",
            "step 900: train loss 2.1241, val loss 2.1504\n",
            "step 1000: train loss 2.1036, val loss 2.1306\n",
            "step 1100: train loss 2.0698, val loss 2.1180\n",
            "step 1200: train loss 2.0380, val loss 2.0791\n",
            "step 1300: train loss 2.0248, val loss 2.0634\n",
            "step 1400: train loss 1.9926, val loss 2.0359\n",
            "step 1500: train loss 1.9697, val loss 2.0287\n",
            "step 1600: train loss 1.9627, val loss 2.0477\n",
            "step 1700: train loss 1.9403, val loss 2.0115\n",
            "step 1800: train loss 1.9090, val loss 1.9941\n",
            "step 1900: train loss 1.9092, val loss 1.9858\n",
            "step 2000: train loss 1.8847, val loss 1.9925\n",
            "step 2100: train loss 1.8724, val loss 1.9757\n",
            "step 2200: train loss 1.8580, val loss 1.9594\n",
            "step 2300: train loss 1.8560, val loss 1.9537\n",
            "step 2400: train loss 1.8412, val loss 1.9427\n",
            "step 2500: train loss 1.8141, val loss 1.9402\n",
            "step 2600: train loss 1.8292, val loss 1.9397\n",
            "step 2700: train loss 1.8116, val loss 1.9322\n",
            "step 2800: train loss 1.8032, val loss 1.9218\n",
            "step 2900: train loss 1.8022, val loss 1.9285\n",
            "step 3000: train loss 1.7955, val loss 1.9195\n",
            "step 3100: train loss 1.7672, val loss 1.9192\n",
            "step 3200: train loss 1.7568, val loss 1.9138\n",
            "step 3300: train loss 1.7551, val loss 1.9059\n",
            "step 3400: train loss 1.7549, val loss 1.8945\n",
            "step 3500: train loss 1.7383, val loss 1.8956\n",
            "step 3600: train loss 1.7242, val loss 1.8868\n",
            "step 3700: train loss 1.7273, val loss 1.8822\n",
            "step 3800: train loss 1.7176, val loss 1.8923\n",
            "step 3900: train loss 1.7219, val loss 1.8750\n",
            "step 4000: train loss 1.7131, val loss 1.8603\n",
            "step 4100: train loss 1.7105, val loss 1.8777\n",
            "step 4200: train loss 1.7033, val loss 1.8675\n",
            "step 4300: train loss 1.7038, val loss 1.8556\n",
            "step 4400: train loss 1.7057, val loss 1.8643\n",
            "step 4500: train loss 1.6875, val loss 1.8528\n",
            "step 4600: train loss 1.6887, val loss 1.8405\n",
            "step 4700: train loss 1.6834, val loss 1.8501\n",
            "step 4800: train loss 1.6675, val loss 1.8437\n",
            "step 4900: train loss 1.6684, val loss 1.8407\n",
            "step 4999: train loss 1.6645, val loss 1.8286\n",
            "\n",
            "\n",
            "KING RICHARD II:\n",
            "Shal lifest made to bub, to take Our my dagatants:\n",
            "Whith foul his vetward that a endrer, my fears' to zorm heavens,\n",
            "Oof it heart my would but\n",
            "With ensengmin latest in ov the doest not.\n",
            "\n",
            "WARWICK:\n",
            "Welll now, and thus quechiry: there's speak you love.\n",
            "In Bodiet, and whom the sclittle\n",
            "Enout-now what evily well most rive with is compon to the me\n",
            "Town danters, If so;\n",
            "Ange to shall do aleous, for dear?\n",
            "\n",
            "KING HENRY VI:\n",
            "Hark, but a\n",
            "ards bring Edward?\n",
            "\n",
            "GROKE:\n",
            "As is no Rurnts I am you! who neet.\n",
            "Pom mary thou contrantym so a thense.\n",
            "\n",
            "QUEEN VINCENTIO:\n",
            "O, sir, may in God't well ow, whom confessy.\n",
            "Which migh.\n",
            "\n",
            "ARCHILINIUS:\n",
            "Dithul seaze Peed me: very it passce of's cruport;\n",
            "How what make you fear tals: there loves\n",
            "Tunkistren in deed, is xment.\n",
            "\n",
            "CORIONIUS:\n",
            "What comforts me. I with self From the walt I?\n",
            "\n",
            "GRINION:\n",
            "Which ushold.\n",
            "\n",
            "KING HENRY Gindner:\n",
            "Withrief I doot, is onter now.\n",
            "\n",
            "Securming:\n",
            "Intande whose no crown some Eiverely marry sold;\n",
            "For for me watch the\n",
            "our torguet! Goy, know our her and brut what I, I huself as humsell.\n",
            "\n",
            "APTOLYCUM:\n",
            "Laitance and toarth or word\n",
            "As beherefitions so me worting.\n",
            "\n",
            "CORIOLINA:\n",
            "What a wouldds,\n",
            "An but branedy wouldIng my a canity:\n",
            "Was you be any in Becausing watcess the Regreast men is what see would in thas jury your Hrannertandless;\n",
            "As there'erliacter me band frind through he crown, I she love is stay just torment:\n",
            "Slaw you behoth unserving of vonby the post,\n",
            "Whave baste hold; I they nengety may's fries\n",
            "To there's fince, I heave arrow old,\n",
            "Thee best sincess soul be\n",
            "that Lord, as;\n",
            "River thou a-latsteer:\n",
            "Out.\n",
            "\n",
            "PORALLINA:\n",
            "Where but\n",
            "Braight gentle, drieven the know you\n",
            "for that to this mack a rishn. Prawity arm as is infectely,\n",
            "Ah, sinstats o' no, this send; commant to love,\n",
            "Go fly this fathal\n",
            "I cortuns cold, offrong to old, the courtly thee? before a gace.\n",
            "\n",
            "KING RICHARD III:\n",
            "A life he pusict\n",
            "It. Vitters, and were not fanturs, thy promind thy awonse than a braute comforn,\n",
            "Will Roman! you brain shown'd for a dresss me; he heavison!\n",
            "\n",
            "\n",
            "MENE\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fjjvMifYZf7x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}