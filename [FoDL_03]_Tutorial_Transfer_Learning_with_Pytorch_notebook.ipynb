{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYu2cT6mfG_A"
      },
      "source": [
        "This tutorial is mainly based on learnpytorch.io's tutorial. VietAI team added DeepDream & Adversarial attack on the second half of the tutorial\n",
        "\n",
        "## What is transfer learning?\n",
        "\n",
        "Transfer learning allows us to take the patterns (also called weights) another model has learned from another problem and use them for our own problem.\n",
        "\n",
        "For example, we can take the patterns a computer vision model has learned from datasets such as ImageNet (millions of images of different objects) and use them to power our Corgi vs Shiba classifier :)\n",
        "\n",
        "Or we could take the patterns from a language model (a model that's been through large amounts of text to learn a representation of language) and use them as the basis of a model to classify different text samples.\n",
        "\n",
        "In short: find a well-performing existing model and apply it to your own problem.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-transfer-learning-example-overview.png\" alt=\"transfer learning overview on different problems\" width=900/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-j5Crv-jgD0"
      },
      "source": [
        "## Why use transfer learning?\n",
        "\n",
        "There are two main benefits to using transfer learning:\n",
        "\n",
        "1. Can leverage an existing model (usually a neural network architecture) proven to work on problems similar to our own.\n",
        "2. Can leverage a working model which has **already learned** patterns on similar data to our own. This often results in achieving **great results with less custom data**.\n",
        "\n",
        "We'll be putting these to the test for the FoodVision Mini problem, we'll take a computer vision model pretrained on ImageNet and try to leverage its underlying learned representations for classifying images of pizza, steak and sushi.\n",
        "\n",
        "\n",
        "Jeremy Howard (founder of [fastai](https://www.fast.ai/)) is a big proponent of transfer learning.\n",
        "\n",
        "> The things that really make a difference (transfer learning), if we can do better at transfer learning, it’s this world changing thing. Suddenly lots more people can do world-class work with less resources and less data. — [Jeremy Howard on the Lex Fridman Podcast](https://youtu.be/Bi7f1JSSlh8?t=72)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qyp6RsYKlUgP"
      },
      "source": [
        "## Where to find pretrained models\n",
        "\n",
        "Often, code and pretrained models for the latest state-of-the-art research is released within a few days of publishing.\n",
        "\n",
        "And there are several places you can find pretrained models to use for your own problems.\n",
        "\n",
        "| **Location** | **What's there?** | **Link(s)** |\n",
        "| ----- | ----- | ----- |\n",
        "| **PyTorch domain libraries** | Each of the PyTorch domain libraries (`torchvision`, `torchtext`) come with pretrained models of some form. The models there work right within PyTorch. | [`torchvision.models`](https://pytorch.org/vision/stable/models.html), [`torchtext.models`](https://pytorch.org/text/main/models.html), [`torchaudio.models`](https://pytorch.org/audio/stable/models.html), [`torchrec.models`](https://pytorch.org/torchrec/torchrec.models.html) |\n",
        "| **HuggingFace Hub** | A series of pretrained models on many different domains (vision, text, audio and more) from organizations around the world. There's plenty of different datasets too. | https://huggingface.co/models, https://huggingface.co/datasets |\n",
        "| **`timm` (PyTorch Image Models) library** | Almost all of the latest and greatest computer vision models in PyTorch code as well as plenty of other helpful computer vision features. | https://github.com/rwightman/pytorch-image-models|\n",
        "| **Paperswithcode** | A collection of the latest state-of-the-art machine learning papers with code implementations attached. You can also find benchmarks here of model performance on different tasks. | https://paperswithcode.com/ |\n",
        "\n",
        "\n",
        "*With access to such high-quality resources as above, it should be common practice at the start of every deep learning problem you take on to ask, \"Does a pretrained model exist for my problem?\"*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXHhqlvnqIY1"
      },
      "source": [
        "## What we're going to cover\n",
        "\n",
        "We're going to take a pretrained model from `torchvision.models` and customise it to work on (and hopefully improve) our FoodVision Mini problem.\n",
        "\n",
        "| **Topic** | **Contents** |\n",
        "| ----- | ----- |\n",
        "| **0. Getting setup** | We've written a fair bit of useful code over the past few sections, let's download it and make sure we can use it again. |\n",
        "| **1. Get data** | Let's get the pizza, steak and sushi image classification dataset we've been using to try and improve our model's results. |\n",
        "| **2. Create Datasets and DataLoaders** | Setup our DataLoaders. |\n",
        "| **3. Get and customise a pretrained model** | Here we'll download a pretrained model from `torchvision.models` and customise it to our own problem. |\n",
        "| **4. Train model** | Let's see how the new pretrained model goes on our pizza, steak, sushi dataset. We'll use the training functions we created in the previous chapter. |\n",
        "| **5. Evaluate the model by plotting loss curves** | How did our first transfer learning model go? Did it overfit or underfit?  |\n",
        "| **6. Make predictions on images from the test set** | It's one thing to check out a model's evaluation metrics but it's another thing to view its predictions on test samples, let's *visualize, visualize, visualize*! |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D4m3Dzbtqi51"
      },
      "source": [
        "## 0. Getting setup\n",
        "\n",
        "Let's get started by importing/downloading the required modules for this section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7J4yH82hqnia"
      },
      "outputs": [],
      "source": [
        "# Continue with regular imports\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "from torch import nn\n",
        "from torchvision import transforms\n",
        "\n",
        "# Try to get torchinfo, install it if it doesn't work\n",
        "try:\n",
        "    from torchinfo import summary\n",
        "except:\n",
        "    print(\"[INFO] Couldn't find torchinfo... installing it.\")\n",
        "    !pip install -q torchinfo\n",
        "    from torchinfo import summary\n",
        "\n",
        "# Try to import the going_modular directory, download it from GitHub if it doesn't work\n",
        "try:\n",
        "    from going_modular.going_modular import data_setup, engine\n",
        "except:\n",
        "    # Get the going_modular scripts\n",
        "    print(\"[INFO] Couldn't find going_modular scripts... downloading them from GitHub.\")\n",
        "    !git clone https://github.com/mrdbourke/pytorch-deep-learning\n",
        "    !mv pytorch-deep-learning/going_modular .\n",
        "    !rm -rf pytorch-deep-learning\n",
        "    from going_modular.going_modular import data_setup, engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U-dVCTk_rl1w"
      },
      "outputs": [],
      "source": [
        "# Setup device agnostic code\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haV3svleroRg"
      },
      "source": [
        "## 1. Get data\n",
        "\n",
        "Before we can start to use **transfer learning**, we'll need a dataset.\n",
        "\n",
        "Let's write some code to download the [`pizza_steak_sushi.zip`](https://github.com/mrdbourke/pytorch-deep-learning/blob/main/data/pizza_steak_sushi.zip) dataset from the course GitHub and then unzip it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSTFxZbIrfgQ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "\n",
        "# Setup path to data folder\n",
        "data_path = Path(\"data/\")\n",
        "image_path = data_path / \"pizza_steak_sushi\"\n",
        "\n",
        "# If the image folder doesn't exist, download it and prepare it...\n",
        "if image_path.is_dir():\n",
        "    print(f\"{image_path} directory exists.\")\n",
        "else:\n",
        "    print(f\"Did not find {image_path} directory, creating one...\")\n",
        "    image_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Download pizza, steak, sushi data\n",
        "    with open(data_path / \"pizza_steak_sushi.zip\", \"wb\") as f:\n",
        "        request = requests.get(\"https://github.com/mrdbourke/pytorch-deep-learning/raw/main/data/pizza_steak_sushi.zip\")\n",
        "        print(\"Downloading pizza, steak, sushi data...\")\n",
        "        f.write(request.content)\n",
        "\n",
        "    # Unzip pizza, steak, sushi data\n",
        "    with zipfile.ZipFile(data_path / \"pizza_steak_sushi.zip\", \"r\") as zip_ref:\n",
        "        print(\"Unzipping pizza, steak, sushi data...\")\n",
        "        zip_ref.extractall(image_path)\n",
        "\n",
        "    # Remove .zip file\n",
        "    os.remove(data_path / \"pizza_steak_sushi.zip\")\n",
        "\n",
        "# Setup Dirs\n",
        "train_dir = image_path / \"train\"\n",
        "test_dir = image_path / \"test\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tan5dOFjsDy-"
      },
      "source": [
        "## 2. Create Datasets and DataLoaders\n",
        "\n",
        "Since we'll be using a pretrained model from [`torchvision.models`](https://pytorch.org/vision/stable/models.html), there's a specific transform we need to prepare our images first."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7clD5ikSsLPf"
      },
      "source": [
        "### 2.1 Creating a transform for `torchvision.models` (manual creation)\n",
        "\n",
        "When we use a pretrained model, it's important that **your custom data going into the model is prepared in the same way as the original training data that went into the model**.\n",
        "\n",
        "Let's compose a series of `torchvision.transforms` to perform these steps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "trKTn_EOr85Q"
      },
      "outputs": [],
      "source": [
        "# Create a transforms pipeline manually (required for torchvision < 0.13)\n",
        "manual_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)), # 1. Reshape all images to 224x224 (though some models may require different sizes)\n",
        "    transforms.ToTensor(), # 2. Turn image values to between 0 & 1\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406], # 3. A mean of [0.485, 0.456, 0.406] (across each colour channel)\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    ) # 4. A standard deviation of [0.229, 0.224, 0.225] (across each colour channel),\n",
        "])\n",
        "\n",
        "manual_transforms"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFR3tzs8_NUo"
      },
      "source": [
        "\n",
        "***Challenge***: *Where did the mean and standard deviation values come from? Why do we need to do this?*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tyzovVEdMGw"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "img = Image.open(\"/content/data/pizza_steak_sushi/train/pizza/1008844.jpg\")\n",
        "img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGvfIBGNdSYF"
      },
      "outputs": [],
      "source": [
        "img_tensor = manual_transforms(img)\n",
        "print(type(img_tensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5iN9JNwcdYhs"
      },
      "outputs": [],
      "source": [
        "img_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVHT9R4M_WNP"
      },
      "source": [
        "### 2.2 Creating a transform for `torchvision.models` (auto creation)\n",
        "\n",
        "Above we saw how to manually create a transform for a pretrained model. But as of `torchvision` v0.13+, an automatic transform creation feature has been added.\n",
        "\n",
        "When you setup a model from `torchvision.models` and select the pretrained model weights you'd like to use, for example, say we'd like to use:\n",
        "    \n",
        "```python\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT\n",
        "```\n",
        "\n",
        "Where,\n",
        "* `EfficientNet_B0_Weights` is the model architecture weights we'd like to use (there are many differnt model architecture options in `torchvision.models`).\n",
        "* `DEFAULT` means the *best available* weights (the best performance in ImageNet).\n",
        "    * **Note:** Depending on the model architecture you choose, you may also see other options such as `IMAGENET_V1` and `IMAGENET_V2` where generally the higher version number the better. Though if you want the best available, `DEFAULT` is the easiest option. See the [`torchvision.models` documentation](https://pytorch.org/vision/main/models.html) for more.\n",
        "    \n",
        "To access the transforms assosciated with our `weights`, we can use the `transforms()` method.\n",
        "\n",
        "This is essentially saying \"get the data transforms that were used to train the `EfficientNet_B0_Weights` on ImageNet\".\n",
        "\n",
        "Let's try it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLo6YCRetXrs"
      },
      "outputs": [],
      "source": [
        "# Get a set of pretrained model weights\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights from pretraining on ImageNet\n",
        "\n",
        "# Get the transforms used to create our pretrained weights\n",
        "auto_transforms = weights.transforms()\n",
        "auto_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hklFAABVd0V4"
      },
      "outputs": [],
      "source": [
        "auto_transforms(img).shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiC94mqTCKLI"
      },
      "source": [
        "The benefit of automatically creating a transform through `weights.transforms()` is that you ensure you're using the same data transformation as the pretrained model used when it was trained.\n",
        "\n",
        "However, the tradeoff of using automatically created transforms is a lack of customization.\n",
        "\n",
        "We can use `auto_transforms` to create DataLoaders with `create_dataloaders()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5y4m_ev5ByMy"
      },
      "outputs": [],
      "source": [
        "# Create training and testing DataLoaders as well as get a list of class names\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=manual_transforms, # resize, convert images to between 0 & 1 and normalize them\n",
        "    batch_size=32\n",
        ") # set mini-batch size to 32\n",
        "\n",
        "train_dataloader, test_dataloader, class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RDJGKkNJenCm"
      },
      "outputs": [],
      "source": [
        "from torchvision import datasets, transforms\n",
        "\n",
        "train_data = datasets.ImageFolder(train_dir, transform=manual_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFLNCDDLesD9"
      },
      "outputs": [],
      "source": [
        "train_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KTzTIkRsewWN"
      },
      "outputs": [],
      "source": [
        "train_data[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOM5qHc7fudJ"
      },
      "outputs": [],
      "source": [
        "batch = next(iter(train_dataloader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3vKOO05DfzgU"
      },
      "outputs": [],
      "source": [
        "len(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gmlh1ugJf2Xf"
      },
      "outputs": [],
      "source": [
        "batch[0].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45eXqi6of6Gz"
      },
      "outputs": [],
      "source": [
        "batch[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ijpn_oczCkDv"
      },
      "source": [
        "## 3. Getting a pretrained model\n",
        "\n",
        "Here comes the fun part!\n",
        "\n",
        "Over the past few notebooks we've been building neural networks from scratch.\n",
        "\n",
        "And while that's a good skill to have, our models haven't been performing as well as we'd like.\n",
        "\n",
        "That's where **transfer learning** comes in.\n",
        "\n",
        "The whole idea of transfer learning is to **take an already well-performing model on a problem-space similar to yours and then customising it to your use case**.\n",
        "\n",
        "Since we're working on a computer vision problem (image classification with FoodVision Mini), we can find pretrained classification models in [`torchvision.models`](https://pytorch.org/vision/stable/models.html#classification).\n",
        "\n",
        "Exploring the documentation, you'll find plenty of common computer vision architecture backbones such as:\n",
        "\n",
        "| **Architecuture backbone** | **Code** |\n",
        "| ----- | ----- |\n",
        "| [ResNet](https://arxiv.org/abs/1512.03385)'s | `torchvision.models.resnet18()`, `torchvision.models.resnet50()`... |\n",
        "| [VGG](https://arxiv.org/abs/1409.1556) | `torchvision.models.vgg16()` |\n",
        "| [EfficientNet](https://arxiv.org/abs/1905.11946)'s | `torchvision.models.efficientnet_b0()`, `torchvision.models.efficientnet_b1()`... |\n",
        "| [VisionTransformer](https://arxiv.org/abs/2010.11929) (ViT's)| `torchvision.models.vit_b_16()`, `torchvision.models.vit_b_32()`... |\n",
        "| [ConvNeXt](https://arxiv.org/abs/2201.03545) | `torchvision.models.convnext_tiny()`,  `torchvision.models.convnext_small()`... |\n",
        "| More available in `torchvision.models` | `torchvision.models...` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96ZTjTqXC3Lc"
      },
      "source": [
        "### 3.1 Which pretrained model should you use?\n",
        "\n",
        "It depends on your problem/the device you're working with.\n",
        "\n",
        "Generally, the higher number in the model name (e.g. `efficientnet_b0()` -> `efficientnet_b1()` -> `efficientnet_b7()`) means *better performance* but a *larger* model.\n",
        "\n",
        "You might think better performance is *always better*, right?\n",
        "\n",
        "That's true but **some better performing models are too big for some devices**.\n",
        "\n",
        "For example, say you'd like to run your model on a mobile-device, you'll have to take into account the limited compute resources on the device, thus you'd be looking for a smaller model.\n",
        "\n",
        "Understanding this **performance vs. speed vs. size tradeoff** will come with time and practice.\n",
        "\n",
        "`efficientnet_bX` are usually good starter models.\n",
        "\n",
        "> **Note:** Even though we're using `efficientnet_bX`, it's important not to get too attached to any one architecture, as they are always changing as new research gets released. Best to experiment, experiment, experiment and see what works for your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DuzBZcDXELj9"
      },
      "source": [
        "### 3.2 Setting up a pretrained model\n",
        "\n",
        "The pretrained model we're going to be using is [`torchvision.models.efficientnet_b0()`](https://pytorch.org/vision/stable/generated/torchvision.models.efficientnet_b0.html#torchvision.models.efficientnet_b0).\n",
        "\n",
        "The architecture is from the paper *[EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks](https://arxiv.org/abs/1905.11946)*.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-effnet-b0-feature-extractor.png\" alt=\"efficienet_b0 from PyTorch torchvision feature extraction model\" width=900/>\n",
        "\n",
        "*Example of what we're going to create, a pretrained [`EfficientNet_B0` model](https://ai.googleblog.com/2019/05/efficientnet-improving-accuracy-and.html) from `torchvision.models` with the output layer adjusted for our use case of classifying pizza, steak and sushi images.*\n",
        "\n",
        "We can setup the `EfficientNet_B0` pretrained ImageNet weights using the same code as we used to create the transforms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr7eGsE2GN4k"
      },
      "outputs": [],
      "source": [
        "weights = None\n",
        "\n",
        "############################################################################\n",
        "# TODO: setup the EfficientNet_B0 pretrained ImageNet weights using the    #\n",
        "# same code as we used to create the transforms.                           #\n",
        "############################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORO9qjQOGMty"
      },
      "source": [
        "The model has already been trained on millions of images and has a good base representation of image data.\n",
        "\n",
        "The PyTorch version of this pretrained model is capable of achieving ~77.7% accuracy across ImageNet's 1000 classes.\n",
        "\n",
        "We'll also send it to the target device."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBZoCBKfCIoR"
      },
      "outputs": [],
      "source": [
        "from torchvision.models._api import WeightsEnum\n",
        "from torch.hub import load_state_dict_from_url\n",
        "\n",
        "def get_state_dict(self, *args, **kwargs):\n",
        "    kwargs.pop(\"check_hash\")\n",
        "    return load_state_dict_from_url(self.url, *args, **kwargs)\n",
        "WeightsEnum.get_state_dict = get_state_dict\n",
        "\n",
        "weights = torchvision.models.EfficientNet_B0_Weights.DEFAULT # .DEFAULT = best available weights\n",
        "model = torchvision.models.efficientnet_b0(weights=weights).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qXktTpYWixLh"
      },
      "outputs": [],
      "source": [
        "model.features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQC24IldJsG7"
      },
      "source": [
        "If we print the model, we get something similar to the following:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-v2-effnetb0-model-print-out.png\" alt=\"output of printing the efficientnet_b0 model from torchvision.models\" width=900/>\n",
        "\n",
        "Lots and lots and lots of layers.\n",
        "\n",
        "This is one of the benefits of transfer learning, taking an existing model, that's been crafted by some of the best engineers in the world and applying to your own problem.\n",
        "\n",
        "Our `efficientnet_b0` comes in three main parts:\n",
        "1. `features` - A collection of convolutional layers and other various activation layers to learn a base representation of vision data (this base representation/collection of layers is often referred to as **features** or **feature extractor**, \"the base layers of the model learn the different **features** of images\").\n",
        "2. `avgpool` - Takes the average of the output of the `features` layer(s) and turns it into a **feature vector**.\n",
        "3. `classifier` - Turns the **feature vector** into a vector with the same dimensionality as the number of required output classes (since `efficientnet_b0` is pretrained on ImageNet and because ImageNet has 1000 classes, `out_features=1000` is the default)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxzNTDuDJ2JH"
      },
      "source": [
        "### 3.3 Getting a summary of our model with `torchinfo.summary()`\n",
        "\n",
        "To learn more about our model, let's use `torchinfo`'s [`summary()` method](https://github.com/TylerYep/torchinfo#documentation).\n",
        "\n",
        "To do so, we'll pass in:\n",
        " * `model` - the model we'd like to get a summary of.\n",
        " * `input_size` - the shape of the data we'd like to pass to our model, for the case of `efficientnet_b0`, the input size is `(batch_size, 3, 224, 224)`, though other variants of `efficientnet_bX` have different input sizes.\n",
        "    * **Note:** Many modern models can handle input images of varying sizes thanks to [`torch.nn.AdaptiveAvgPool2d()`](https://pytorch.org/docs/stable/generated/torch.nn.AdaptiveAvgPool2d.html), this layer adaptively adjusts the `output_size` of a given input as required. You can try this out by passing different size input images to `summary()` or your models.\n",
        " * `col_names` - the various information columns we'd like to see about our model.\n",
        " * `col_width` - how wide the columns should be for the summary.\n",
        " * `row_settings` - what features to show in a row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rCs23uTVkEq9"
      },
      "outputs": [],
      "source": [
        "model.features[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDoIznVQJh9-"
      },
      "outputs": [],
      "source": [
        "# Print a summary using torchinfo (uncomment for actual output)\n",
        "summary(\n",
        "    model=model,\n",
        "    input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\"\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_09rxiiKJlg"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-torchinfo-summary-unfrozen-layers.png\" alt=\"output of torchinfo.summary() when passed our model with all layers as trainable\" width=900/>\n",
        "\n",
        "That's a big model!\n",
        "\n",
        "From the output of the summary, we can see all of the various input and output shape changes as our image data goes through the model.\n",
        "\n",
        "And there are a whole bunch more total parameters (pretrained weights) to recognize different patterns in our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBuSEpD8Kfg9"
      },
      "source": [
        "### 3.4 Freezing the base model and changing the output layer to suit our needs\n",
        "\n",
        "The process of transfer learning usually goes: freeze some base layers of a pretrained model (typically the `features` section) and then adjust the output layers (also called head/classifier layers) to suit your needs.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-v2-effnet-changing-the-classifier-head.png\" alt=\"changing the efficientnet classifier head to a custom number of outputs\" width=900/>\n",
        "\n",
        "*You can customise the outputs of a pretrained model by changing the output layer(s) to suit your problem. The original `torchvision.models.efficientnet_b0()` comes with `out_features=1000` because there are 1000 classes in ImageNet, the dataset it was trained on. However, for our problem, classifying images of pizza, steak and sushi we only need `out_features=3`.*\n",
        "\n",
        "Let's freeze all of the layers/parameters in the `features` section of our `efficientnet_b0` model.\n",
        "\n",
        "> **Note:** To *freeze* layers means to keep them how they are during training. For example, if your model has pretrained layers, to *freeze* them would be to say, \"don't change any of the patterns in these layers during training, keep them how they are.\" In essence, we'd like to keep the pretrained weights/patterns our model has learned from ImageNet as a backbone and then only change the output layers.\n",
        "\n",
        "We can freeze all of the layers/parameters in the `features` section by setting the attribute `requires_grad=False`.\n",
        "\n",
        "For parameters with `requires_grad=False`, PyTorch doesn't track gradient updates and in turn, these parameters won't be changed by our optimizer during training.\n",
        "\n",
        "In essence, a parameter with `requires_grad=False` is \"untrainable\" or \"frozen\" in place."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nSu_ZkokKFP5"
      },
      "outputs": [],
      "source": [
        "# Freeze all base layers in the \"features\" section of the model (the feature extractor) by setting requires_grad=False\n",
        "for param in model.features.parameters():\n",
        "    param.requires_grad = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUcpmMb6Lss_"
      },
      "source": [
        "Feature extractor layers are frozen!\n",
        "\n",
        "Let's now adjust the output layer or the `classifier` portion of our pretrained model to our needs.\n",
        "\n",
        "Right now our pretrained model has `out_features=1000` because there are 1000 classes in ImageNet.\n",
        "\n",
        "However, we don't have 1000 classes, we only have three, pizza, steak and sushi.\n",
        "\n",
        "We can change the `classifier` portion of our model by creating a new series of layers.\n",
        "\n",
        "The current `classifier` consists of:\n",
        "\n",
        "```\n",
        "(classifier): Sequential(\n",
        "    (0): Dropout(p=0.2, inplace=True)\n",
        "    (1): Linear(in_features=1280, out_features=1000, bias=True)\n",
        "```\n",
        "\n",
        "We'll keep the `Dropout` layer the same using [`torch.nn.Dropout(p=0.2, inplace=True)`](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html).\n",
        "\n",
        "And we'll keep `in_features=1280` for our `Linear` output layer but we'll change the `out_features` value to the length of our `class_names` (`len(['pizza', 'steak', 'sushi']) = 3`).\n",
        "\n",
        "Our new `classifier` layer should be on the same device as our `model`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNNcNyK8LqVo"
      },
      "outputs": [],
      "source": [
        "# Set the manual seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model.classifier = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(\n",
        "        in_features=1280,\n",
        "        out_features=len(class_names), # same number of output units as our number of classes\n",
        "        bias=True)\n",
        ").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gFHTs-UzMQqr"
      },
      "source": [
        "Nice!\n",
        "\n",
        "Output layer updated, let's get another summary of our model and see what's changed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac6y7KAMMJdJ"
      },
      "outputs": [],
      "source": [
        "# # Do a summary *after* freezing the features and changing the output classifier layer (uncomment for actual output)\n",
        "summary(model,\n",
        "        input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "        verbose=0,\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Svavs2QMZLs"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/06-torchinfo-summary-frozen-layers.png\" alt=\"output of torchinfo.summary() after freezing multiple layers in our model and changing the classifier head\" width=900/>\n",
        "\n",
        "Ho, ho! There's a fair few changes here!\n",
        "\n",
        "Let's go through them:\n",
        "* **Trainable column** - You'll see that many of the base layers (the ones in the `features` portion) have their Trainable value as `False`. This is because we set their attribute `requires_grad=False`. Unless we change this, these layers won't be updated during furture training.\n",
        "* **Output shape of `classifier`** - The `classifier` portion of the model now has an Output Shape value of `[32, 3]` instead of `[32, 1000]`. It's Trainable value is also `True`. This means its parameters will be updated during training. In essence, we're using the `features` portion to feed our `classifier` portion a base representation of an image and then our `classifier` layer is going to learn how to base representation aligns with our problem.\n",
        "* **Less trainable parameters** - Previously there was 5,288,548 trainable parameters. But since we froze many of the layers of the model and only left the `classifier` as trainable, there's now only 3,843 trainable parameters (even less than our TinyVGG model). Though there's also 4,007,548 non-trainable parameters, these will create a base representation of our input images to feed into our `classifier` layer.\n",
        "\n",
        "> **Note:** The more trainable parameters a model has, the more compute power/longer it takes to train. Freezing the base layers of our model and leaving it with less trainable parameters means our model should train quite quickly. This is one huge benefit of transfer learning, taking the already learned parameters of a model trained on a problem similar to yours and only tweaking the outputs slightly to suit your problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "As-ECxaEMhjK"
      },
      "source": [
        "## 4. Train model\n",
        "\n",
        "Now we've got a pretraiend model that's semi-frozen and has a customised `classifier`, how about we see transfer learning in action?\n",
        "\n",
        "To begin training, let's create a loss function and an optimizer.\n",
        "\n",
        "Because we're still working with multi-class classification, we'll use `nn.CrossEntropyLoss()` for the loss function.\n",
        "\n",
        "And we'll stick with `torch.optim.Adam()` as our optimizer with `lr=0.001`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CDYZ1GBMTR7"
      },
      "outputs": [],
      "source": [
        "# Define loss and optimizer\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AmKlVNHeNTSt"
      },
      "source": [
        "To train our model, we can use `train()` function below.\n",
        "\n",
        "Let's see how long it takes to train our model for 5 epochs.\n",
        "\n",
        "> **Note:** We're only going to be training the parameters `classifier` here as all of the other parameters in our model have been frozen."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1-5TLktM0GY"
      },
      "outputs": [],
      "source": [
        "from tqdm.auto import tqdm\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "def train(model: torch.nn.Module,\n",
        "          train_dataloader: torch.utils.data.DataLoader,\n",
        "          test_dataloader: torch.utils.data.DataLoader,\n",
        "          optimizer: torch.optim.Optimizer,\n",
        "          loss_fn: torch.nn.Module,\n",
        "          epochs: int,\n",
        "          device: torch.device):\n",
        "    \"\"\"Trains and tests a PyTorch model.\n",
        "    Passes a target PyTorch models through train_step() and test_step()\n",
        "    functions for a number of epochs, training and testing the model\n",
        "    in the same epoch loop.\n",
        "    Calculates, prints and stores evaluation metrics throughout.\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained and tested.\n",
        "    train_dataloader: A DataLoader instance for the model to be trained on.\n",
        "    test_dataloader: A DataLoader instance for the model to be tested on.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n",
        "    epochs: An integer indicating how many epochs to train for.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "    Returns:\n",
        "    A dictionary of training and testing loss as well as training and\n",
        "    testing accuracy metrics. Each metric has a value in a list for\n",
        "    each epoch.\n",
        "    In the form: {train_loss: [...],\n",
        "              train_acc: [...],\n",
        "              test_loss: [...],\n",
        "              test_acc: [...]}\n",
        "    For example if training for epochs=2:\n",
        "             {train_loss: [2.0616, 1.0537],\n",
        "              train_acc: [0.3945, 0.3945],\n",
        "              test_loss: [1.2641, 1.5706],\n",
        "              test_acc: [0.3400, 0.2973]}\n",
        "    \"\"\"\n",
        "    # Create empty results dictionary\n",
        "    results = {\"train_loss\": [],\n",
        "               \"train_acc\": [],\n",
        "               \"test_loss\": [],\n",
        "               \"test_acc\": []\n",
        "    }\n",
        "\n",
        "    # Make sure model on target device\n",
        "    model.to(device)\n",
        "\n",
        "    # Loop through training and testing steps for a number of epochs\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "        train_loss, train_acc = train_step(model=model,\n",
        "                                          dataloader=train_dataloader,\n",
        "                                          loss_fn=loss_fn,\n",
        "                                          optimizer=optimizer,\n",
        "                                          device=device)\n",
        "        test_loss, test_acc = test_step(model=model,\n",
        "          dataloader=test_dataloader,\n",
        "          loss_fn=loss_fn,\n",
        "          device=device)\n",
        "\n",
        "        # Print out what's happening\n",
        "        print(\n",
        "          f\"Epoch: {epoch+1} | \"\n",
        "          f\"train_loss: {train_loss:.4f} | \"\n",
        "          f\"train_acc: {train_acc:.4f} | \"\n",
        "          f\"test_loss: {test_loss:.4f} | \"\n",
        "          f\"test_acc: {test_acc:.4f}\"\n",
        "        )\n",
        "\n",
        "        # Update results dictionary\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "    # Return the filled results at the end of the epochs\n",
        "    return results\n",
        "\n",
        "def train_step(model: torch.nn.Module,\n",
        "               dataloader: torch.utils.data.DataLoader,\n",
        "               loss_fn: torch.nn.Module,\n",
        "               optimizer: torch.optim.Optimizer,\n",
        "               device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Trains a PyTorch model for a single epoch.\n",
        "    Turns a target PyTorch model to training mode and then\n",
        "    runs through all of the required training steps (forward\n",
        "    pass, loss calculation, optimizer step).\n",
        "    Args:\n",
        "    model: A PyTorch model to be trained.\n",
        "    dataloader: A DataLoader instance for the model to be trained on.\n",
        "    loss_fn: A PyTorch loss function to minimize.\n",
        "    optimizer: A PyTorch optimizer to help minimize the loss function.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "    Returns:\n",
        "    A tuple of training loss and training accuracy metrics.\n",
        "    In the form (train_loss, train_accuracy). For example:\n",
        "    (0.1112, 0.8743)\n",
        "    \"\"\"\n",
        "    # Put model in train mode\n",
        "    model.train()\n",
        "\n",
        "    # Setup train loss and train accuracy values\n",
        "    train_loss, train_acc = 0, 0\n",
        "\n",
        "    # Loop through data loader data batches\n",
        "    for batch_index, (X, y) in enumerate(dataloader):\n",
        "        # Send data to target device\n",
        "        X, y = X.to(device), y.to(device)\n",
        "\n",
        "        # 1. Forward pass\n",
        "        y_pred = model(X)\n",
        "\n",
        "        # 2. Calculate and accumulate loss\n",
        "        loss = loss_fn(y_pred, y)\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        # 3. Optimizer zero grad\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # 4. Loss backward\n",
        "        loss.backward()\n",
        "\n",
        "        # 5. Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        # Calculate and accumulate accuracy metric across all batches\n",
        "        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
        "        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    train_loss = train_loss / len(dataloader)\n",
        "    train_acc = train_acc / len(dataloader)\n",
        "    return train_loss, train_acc\n",
        "\n",
        "def test_step(model: torch.nn.Module,\n",
        "              dataloader: torch.utils.data.DataLoader,\n",
        "              loss_fn: torch.nn.Module,\n",
        "              device: torch.device) -> Tuple[float, float]:\n",
        "    \"\"\"Tests a PyTorch model for a single epoch.\n",
        "    Turns a target PyTorch model to \"eval\" mode and then performs\n",
        "    a forward pass on a testing dataset.\n",
        "    Args:\n",
        "    model: A PyTorch model to be tested.\n",
        "    dataloader: A DataLoader instance for the model to be tested on.\n",
        "    loss_fn: A PyTorch loss function to calculate loss on the test data.\n",
        "    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n",
        "    Returns:\n",
        "    A tuple of testing loss and testing accuracy metrics.\n",
        "    In the form (test_loss, test_accuracy). For example:\n",
        "    (0.0223, 0.8985)\n",
        "    \"\"\"\n",
        "    # Put model in eval mode\n",
        "    model.eval()\n",
        "\n",
        "    # Setup test loss and test accuracy values\n",
        "    test_loss, test_acc = 0, 0\n",
        "\n",
        "    # Turn on inference context manager\n",
        "    with torch.inference_mode():\n",
        "        # Loop through DataLoader batches\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "            # Send data to target device\n",
        "            X, y = X.to(device), y.to(device)\n",
        "\n",
        "            # 1. Forward pass\n",
        "            test_pred_logits = model(X)\n",
        "\n",
        "            # 2. Calculate and accumulate loss\n",
        "            loss = loss_fn(test_pred_logits, y)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "            # Calculate and accumulate accuracy\n",
        "            test_pred_labels = test_pred_logits.argmax(dim=1)\n",
        "            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n",
        "\n",
        "    # Adjust metrics to get average loss and accuracy per batch\n",
        "    test_loss = test_loss / len(dataloader)\n",
        "    test_acc = test_acc / len(dataloader)\n",
        "    return test_loss, test_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ni96YX0HMmo-"
      },
      "outputs": [],
      "source": [
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "results = train(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=5,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NtLQstYIR_3w"
      },
      "source": [
        "Wow!\n",
        "\n",
        "Our model is trained very fast (about 10 seconds on Google Colab with a T4 GPU).\n",
        "\n",
        "With an `efficientnet_b0` backbone, our model achieves almost 90% accuracy on the test dataset. Not bad for a model we downloaded with a few lines of code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8dtxkPkYE1P"
      },
      "source": [
        "Now, let's see how the `efficientnet_b0` architecture performs without using weights trained on ImageNet but randomly initialized from scratch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1PXX2_xnN_8C"
      },
      "outputs": [],
      "source": [
        "nontransfer_model = torchvision.models.efficientnet_b0().to(device)\n",
        "\n",
        "# Should not freeze model in this case. Why?\n",
        "# for param in nontransfer_model.features.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "# Define loss and optimizer\n",
        "optimizer = torch.optim.Adam(nontransfer_model.parameters(), lr=0.001)\n",
        "\n",
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "# Setup training and save the results\n",
        "\n",
        "\n",
        "results = train(\n",
        "    model=nontransfer_model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=5,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OeuHQDtUYfNc"
      },
      "source": [
        "It's really, really bad without transfer learning :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bhFLQUaTS2WU"
      },
      "source": [
        "## 5. Make predictions on images from the test set\n",
        "\n",
        "It looks like our model performs well quantitatively but how about qualitatively?\n",
        "\n",
        "Let's find out by making some predictions with our model on images from the test set (these aren't seen during training) and plotting them.\n",
        "\n",
        "*Visualize, visualize, visualize!*\n",
        "\n",
        "One thing we'll have to remember is that for our model to make predictions on an image, the image has to be in *same* format as the images our model was trained on.\n",
        "\n",
        "This means we'll need to make sure our images have:\n",
        "* **Same shape** - If our images are different shapes to what our model was trained on, we'll get shape errors.\n",
        "* **Same datatype** - If our images are a different datatype (e.g. `torch.int8` vs. `torch.float32`) we'll get datatype errors.\n",
        "* **Same device** - If our images are on a different device to our model, we'll get device errors.\n",
        "* **Same transformations** - If our model is trained on images that have been transformed in certain way (e.g. normalized with a specific mean and standard deviation) and we try and make preidctions on images transformed in a different way, these predictions may be off.\n",
        "\n",
        "> **Note:** These requirements go for all kinds of data if you're trying to make predictions with a trained model. Data you'd like to predict on should be in the same format as your model was trained on.\n",
        "\n",
        "To do all of this, we'll create a function `pred_and_plot_image()` to:\n",
        "\n",
        "1. Take in a trained model, a list of class names, a filepath to a target image, an image size, a transform and a target device.\n",
        "2. Open an image with [`PIL.Image.open()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.open).\n",
        "3. Create a transform for the image (this will default to the `manual_transforms` we created above or it could use a transform generated from `weights.transforms()`).\n",
        "4. Make sure the model is on the target device.\n",
        "5. Turn on model eval mode with `model.eval()` (this turns off layers like `nn.Dropout()`, so they aren't used for inference) and the inference mode context manager.\n",
        "6. Transform the target image with the transform made in step 3 and add an extra batch dimension with `torch.unsqueeze(dim=0)` so our input image has shape `[batch_size, color_channels, height, width]`.\n",
        "7. Make a prediction on the image by passing it to the model ensuring it's on the target device.\n",
        "8. Convert the model's output logits to prediction probabilities with `torch.softmax()`.\n",
        "9. Convert model's prediction probabilities to prediction labels with `torch.argmax()`.\n",
        "10. Plot the image with `matplotlib` and set the title to the prediction label from step 9 and prediction probability from step 8."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cB4gbEMgSu_s"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "# 1. Take in a trained model, class names, image path, image size, a transform and target device\n",
        "def pred_and_plot_image(model: torch.nn.Module,\n",
        "                        image_path: str,\n",
        "                        class_names: List[str],\n",
        "                        image_size: Tuple[int, int] = (224, 224),\n",
        "                        transform: torchvision.transforms = None,\n",
        "                        device: torch.device=device):\n",
        "\n",
        "\n",
        "    # 2. Open image\n",
        "    img = Image.open(image_path)\n",
        "\n",
        "    # 3. Create transformation for image (if one doesn't exist)\n",
        "    if transform is not None:\n",
        "        image_transform = transform\n",
        "    else:\n",
        "        image_transform = transforms.Compose([\n",
        "            transforms.Resize(image_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225]),\n",
        "        ])\n",
        "\n",
        "    ### Predict on image ###\n",
        "\n",
        "    # 4. Make sure the model is on the target device\n",
        "    model.to(device)\n",
        "\n",
        "    # 5. Turn on model evaluation mode and inference mode\n",
        "    model.eval()\n",
        "    with torch.inference_mode():\n",
        "      # 6. Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n",
        "      transformed_image = image_transform(img).unsqueeze(dim=0)\n",
        "\n",
        "      # 7. Make a prediction on image with an extra dimension and send it to the target device\n",
        "      target_image_pred = model(transformed_image.to(device))\n",
        "\n",
        "    # 8. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n",
        "    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n",
        "\n",
        "    # 9. Convert prediction probabilities -> prediction labels\n",
        "    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n",
        "\n",
        "    # 10. Plot image with predicted label and probability\n",
        "    plt.figure()\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\")\n",
        "    print(target_image_pred_probs)\n",
        "    plt.axis(False);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IsP0HDfmTQzJ"
      },
      "source": [
        "Let's test it out by making predictions on a few random images from the test set.\n",
        "\n",
        "We can get a list of all the test image paths using `list(Path(test_dir).glob(\"*/*.jpg\"))`, the stars in the `glob()` method say \"any file matching this pattern\", in other words, any file ending in `.jpg` (all of our images).\n",
        "\n",
        "And then we can randomly sample a number of these using Python's [`random.sample(populuation, k)`](https://docs.python.org/3/library/random.html#random.sample) where `population` is the sequence to sample and `k` is the number of samples to retrieve."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtRIWZpOTIVM"
      },
      "outputs": [],
      "source": [
        "# Get a random list of image paths from test set\n",
        "import random\n",
        "num_images_to_plot = 2\n",
        "test_image_path_list = list(Path(test_dir).glob(\"*/*.jpg\")) # get list all image paths from test data\n",
        "test_image_path_sample = random.sample(population=test_image_path_list, # go through all of the test image paths\n",
        "                                       k=num_images_to_plot) # randomly select 'k' image paths to pred and plot\n",
        "\n",
        "# Make predictions on and plot the images\n",
        "for image_path in test_image_path_sample:\n",
        "    pred_and_plot_image(model=model,\n",
        "                        image_path=image_path,\n",
        "                        class_names=class_names,\n",
        "                        # transform=weights.transforms(), # optionally pass in a specified transform from our pretrained model weights\n",
        "                        image_size=(224, 224))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CctlkE2wT8r0"
      },
      "source": [
        "### 5.1 Making predictions on a custom image\n",
        "\n",
        "It looks like our model does well qualitatively on data from the test set.\n",
        "\n",
        "But how about on our own custom image?\n",
        "\n",
        "That's where the real fun of machine learning is!\n",
        "\n",
        "Predicting on your own custom data, outisde of any training or test set.\n",
        "\n",
        "To test our model on a custom image, let's import the old faithful `pizza-dad.jpeg` image.\n",
        "\n",
        "We'll then pass it to the `pred_and_plot_image()` function we created above and see what happens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUeBvka8TTWc"
      },
      "outputs": [],
      "source": [
        "# Download custom image\n",
        "import requests\n",
        "\n",
        "# Setup custom image path\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "\n",
        "# Download the image if it doesn't already exist\n",
        "if not custom_image_path.is_file():\n",
        "    with open(custom_image_path, \"wb\") as f:\n",
        "        # When downloading from GitHub, need to use the \"raw\" file link\n",
        "        request = requests.get(\"https://raw.githubusercontent.com/mrdbourke/pytorch-deep-learning/main/images/04-pizza-dad.jpeg\")\n",
        "        print(f\"Downloading {custom_image_path}...\")\n",
        "        f.write(request.content)\n",
        "else:\n",
        "    print(f\"{custom_image_path} already exists, skipping download.\")\n",
        "\n",
        "# Predict on custom image\n",
        "pred_and_plot_image(model=model,\n",
        "                    image_path=custom_image_path,\n",
        "                    class_names=class_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7siF0Xk4UHyN"
      },
      "source": [
        "Looks like our model go it right again!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5GgOnUiUMD6"
      },
      "source": [
        "## Main takeaways\n",
        "* **Transfer learning** often allows to you get good results with a relatively small amount of custom data.\n",
        "* Knowing the power of transfer learning, it's a good idea to ask at the start of every problem, \"does an existing well-performing model exist for my problem?\"\n",
        "* When using a pretrained model, it's important that your custom data be formatted/preprocessed in the same way that the original model was trained on, otherwise you may get degraded performance.\n",
        "* The same goes for predicting on custom data, ensure your custom data is in the same format as the data your model was trained on.\n",
        "* There are several different places to find pretrained models from the PyTorch domain libraries, HuggingFace Hub and libraries such as `timm` (PyTorch Image Models)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pQ-oFeeaAZw"
      },
      "source": [
        "# Quiz: Beat EfficientNet using ResNet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uiHmpAubc2z"
      },
      "source": [
        "Choose a ResNet architecture and leverage transfer learning to beat the performance of EfficientNet_B0 above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2HN79ZxlqK9V"
      },
      "outputs": [],
      "source": [
        "del model\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2wz-I54UaE4M"
      },
      "outputs": [],
      "source": [
        "############################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################\n",
        "\n",
        "# Sanity check\n",
        "assert model(preprocess(img).unsqueeze(0).to(device)).shape[1] == len(class_names), \"Something wrong with my forward function\"\n",
        "\n",
        "summary(\n",
        "    model,\n",
        "    input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "    verbose=0,\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_67_lVa3qmdg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qS07ig-_cRSN"
      },
      "outputs": [],
      "source": [
        "weights = torchvision.models.ResNet50_Weights.DEFAULT # .DEFAULT = best available weights\n",
        "model = torchvision.models.resnet50(weights=weights).to(device)\n",
        "preprocess = weights.transforms()\n",
        "\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "for param in model.layer4.parameters():\n",
        "    param.requires_grad = True\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "# Recreate the classifier layer and seed it to the target device\n",
        "model.fc = torch.nn.Sequential(\n",
        "    torch.nn.Dropout(p=0.2, inplace=True),\n",
        "    torch.nn.Linear(\n",
        "        in_features=2048,\n",
        "        out_features=len(class_names), # same number of output units as our number of classes\n",
        "        bias=True)\n",
        ").to(device)\n",
        "\n",
        "print(summary(model,\n",
        "    input_size=(32, 3, 224, 224), # make sure this is \"input_size\", not \"input_shape\" (batch_size, color_channels, height, width)\n",
        "    verbose=0,\n",
        "    col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "    col_width=20,\n",
        "    row_settings=[\"var_names\"]\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SppGNWWmsOmC"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Set the random seeds\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed(42)\n",
        "\n",
        "train_dataloader, test_dataloader, class_names = data_setup.create_dataloaders(\n",
        "    train_dir=train_dir,\n",
        "    test_dir=test_dir,\n",
        "    transform=preprocess, # resize, convert images to between 0 & 1 and normalize them\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Start the timer\n",
        "from timeit import default_timer as timer\n",
        "start_time = timer()\n",
        "\n",
        "\n",
        "# Setup training and save the results\n",
        "results = train(\n",
        "    model=model,\n",
        "    train_dataloader=train_dataloader,\n",
        "    test_dataloader=test_dataloader,\n",
        "    optimizer=optimizer,\n",
        "    loss_fn=loss_fn,\n",
        "    epochs=5,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "# End the timer and print out how long it took\n",
        "end_time = timer()\n",
        "print(f\"[INFO] Total training time: {end_time-start_time:.3f} seconds\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kpFE_ITkdLRB"
      },
      "outputs": [],
      "source": [
        "# 5. Make predictions on images from the test set (10 points)\n",
        "\n",
        "############################################################################\n",
        "# *****START OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "\n",
        "# *****END OF YOUR CODE (DO NOT DELETE/MODIFY THIS LINE)*****\n",
        "############################################################################\n",
        "#                             END OF YOUR CODE                             #\n",
        "############################################################################"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs1hPM0idQ6y"
      },
      "outputs": [],
      "source": [
        "# 5.1 Making predictions on a custom image\n",
        "\n",
        "custom_image_path = './cow.jpg'\n",
        "\n",
        "custom_url = \"https://cdn.britannica.com/55/174255-050-526314B6/brown-Guernsey-cow.jpg\"\n",
        "\n",
        "\n",
        "with open(custom_image_path, \"wb\") as f:\n",
        "#     # When downloading from GitHub, need to use the \"raw\" file link\n",
        "    request = requests.get(custom_url)\n",
        "    print(f\"Downloading {custom_image_path}...\")\n",
        "    f.write(request.content)\n",
        "\n",
        "\n",
        "pred_and_plot_image(\n",
        "    model=model,\n",
        "    image_path=custom_image_path,\n",
        "    class_names=class_names\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shiny new things! Visualize trained model with DeepDream"
      ],
      "metadata": {
        "id": "cfe01E6NFWIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torchvision.models.VGG16_Weights.DEFAULT\n",
        "model_vgg16 = torchvision.models.vgg16(weights=weights).to(device)\n",
        "preprocess_vgg16 = weights.transforms()"
      ],
      "metadata": {
        "id": "CYLMuhl6hP0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess_vgg16"
      ],
      "metadata": {
        "id": "s5RFyX9Ehk2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (1 - mean)/std >= (x - mean)/std >= 0 - mean/std\n",
        "mean, std = np.array(preprocess_vgg16.mean), np.array(preprocess_vgg16.std)\n",
        "LOWER_IMAGE_BOUND = torch.tensor((-mean / std).reshape(1, -1, 1, 1)).to(device)\n",
        "UPPER_IMAGE_BOUND = torch.tensor(((1 - mean) / std).reshape(1, -1, 1, 1)).to(device)"
      ],
      "metadata": {
        "id": "sxROye_uSuFX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_np(tensor):\n",
        "    tensor = tensor.clone().cpu()\n",
        "    mean = torch.tensor(preprocess_vgg16.mean).view(3, 1, 1)\n",
        "    std = torch.tensor(preprocess_vgg16.std).view(3, 1, 1)\n",
        "    tensor.mul_(std).add_(mean)\n",
        "    tensor = tensor.mul(255).byte()[0].permute(1, 2, 0)\n",
        "    return tensor.numpy()"
      ],
      "metadata": {
        "id": "4YSpXYfkPxzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "random_tensor = torch.rand(size=(1, 3, 224, 224), requires_grad=True, device=device)\n",
        "\n",
        "print(\">>>>>>\", random_tensor.requires_grad)\n",
        "\n",
        "opt = optim.Adam([random_tensor], lr=1e-1)\n",
        "\n",
        "for t in range(1000):\n",
        "    pred = model_vgg16(random_tensor)\n",
        "    loss = nn.CrossEntropyLoss()(pred, torch.LongTensor([151]).to(device))\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    if t % 200 == 0:\n",
        "        print(t, loss.item())\n",
        "        tensor_to_np(random_tensor)\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor_to_np(random_tensor))\n",
        "        plt.title(f\"Step {t}\")\n",
        "\n",
        "    random_tensor.data.clamp_(LOWER_IMAGE_BOUND, UPPER_IMAGE_BOUND)\n",
        "\n",
        "tensor_to_np(random_tensor)\n",
        "plt.figure()\n",
        "plt.imshow(tensor_to_np(random_tensor))\n",
        "plt.title(f\"Step {t}\")"
      ],
      "metadata": {
        "id": "tNoA9Vo6Fem7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "custom_image_path = \"/content/data/pizza_steak_sushi/test/pizza/1503858.jpg\"\n",
        "img = Image.open(custom_image_path)\n",
        "\n",
        "random_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "random_tensor.requires_grad_()\n",
        "print(random_tensor.shape)\n",
        "\n",
        "print(\">>>>>>\", random_tensor.requires_grad)\n",
        "\n",
        "opt = optim.Adam([random_tensor], lr=1e-1)\n",
        "\n",
        "for t in range(1000):\n",
        "    pred = model_vgg16(random_tensor)\n",
        "    loss = nn.CrossEntropyLoss()(pred, torch.LongTensor([151]).to(device))\n",
        "\n",
        "    if t % 200 == 0:\n",
        "        print(t, loss.item())\n",
        "        tensor_to_np(random_tensor)\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor_to_np(random_tensor))\n",
        "        plt.title(f\"Step {t}\")\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    random_tensor.data.clamp_(LOWER_IMAGE_BOUND, UPPER_IMAGE_BOUND)\n",
        "\n",
        "tensor_to_np(random_tensor)\n",
        "plt.figure()\n",
        "plt.imshow(tensor_to_np(random_tensor))\n",
        "plt.title(f\"Step {t}\")"
      ],
      "metadata": {
        "id": "duQ3X2FqVKMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We can use pytorch hook\n",
        "\n",
        "class Hook():\n",
        "    def __init__(self, module):\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self.input = input\n",
        "        self.output = output\n",
        "\n",
        "    def close(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "    def reset(self):\n",
        "        del self.input\n",
        "        del self.output\n",
        "        self.input  = None\n",
        "        self.output = None"
      ],
      "metadata": {
        "id": "qe2eLOnPKRxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_vgg16.features"
      ],
      "metadata": {
        "id": "AsS7ZjTiiZ92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then hook it into our models\n",
        "hook = Hook(model_vgg16.features[18])\n",
        "\n",
        "# Test the forward hook\n",
        "model_vgg16(random_tensor)\n",
        "\n",
        "print(hook.output.shape)\n",
        "# Close the hook\n",
        "hook.close()"
      ],
      "metadata": {
        "id": "ufvIosYOiYwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "custom_image_path = \"/content/data/pizza_steak_sushi/test/pizza/1503858.jpg\"\n",
        "\n",
        "img = Image.open(custom_image_path)\n",
        "\n",
        "random_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "random_tensor.requires_grad_()\n",
        "print(random_tensor.shape)\n",
        "\n",
        "print(\">>>>>>\", random_tensor.requires_grad)\n",
        "\n",
        "opt = optim.Adam([random_tensor], lr=1e-2)\n",
        "\n",
        "hook = Hook(model_vgg16.features[22])\n",
        "\n",
        "for t in range(200):\n",
        "    pred = model_vgg16(preprocess(random_tensor))\n",
        "    # Now instead of using this loss, we want to maximize the norm of our layer activation\n",
        "    layer_activation = hook.output\n",
        "    loss = -layer_activation.norm()\n",
        "    # minimize NEGATIVE layer_activation norm => maximize layer norm\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    hook.reset()\n",
        "\n",
        "    if t % 50 == 0:\n",
        "        print(t, loss.item())\n",
        "        tensor_to_np(random_tensor)\n",
        "        plt.figure()\n",
        "        plt.imshow(tensor_to_np(random_tensor))\n",
        "        plt.title(f\"Step {t}\")\n",
        "\n",
        "    random_tensor.data.clamp_(LOWER_IMAGE_BOUND, UPPER_IMAGE_BOUND)\n",
        "\n",
        "hook.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(tensor_to_np(random_tensor))\n",
        "plt.title(f\"Step {t}\")"
      ],
      "metadata": {
        "id": "cfzY3_rnWNum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GvN5-5eVPUlB"
      },
      "source": [
        "# Attacks!!!!!!!!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on: https://adversarial-ml-tutorial.org/introduction/"
      ],
      "metadata": {
        "id": "pKGs9HejfufI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EDsrur-PXKm"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "custom_image_path = data_path / \"04-pizza-dad.jpeg\"\n",
        "img = Image.open(custom_image_path)\n",
        "\n",
        "pizza_tensor = preprocess(img).unsqueeze(0).to(device)\n",
        "print(pizza_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tensor_to_np(tensor):\n",
        "    tensor = tensor.clone().cpu()\n",
        "    mean = torch.tensor(preprocess.mean).view(3, 1, 1)\n",
        "    std = torch.tensor(preprocess.std).view(3, 1, 1)\n",
        "    tensor.mul_(std).add_(mean)\n",
        "    tensor = tensor.mul(255).byte()[0].permute(1, 2, 0)\n",
        "    return tensor.numpy()"
      ],
      "metadata": {
        "id": "ChjB6hvkjO7O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Before adversarial attack\n",
        "\n",
        "pred = model(pizza_tensor)\n",
        "max_class = pred.max(dim=1)[1].item()\n",
        "print(\"Before adversarial attack: Predicted class: \", class_names[max_class])\n",
        "print(\"Predicted probability:\", nn.Softmax(dim=1)(pred)[0,max_class].item())\n",
        "plt.imshow(tensor_to_np(pizza_tensor))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R0XNh_iLeRlK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUIMlOvXQJKN"
      },
      "outputs": [],
      "source": [
        "delta = torch.zeros_like(pizza_tensor, requires_grad=True).to(device)\n",
        "epsilon = 5/255.\n",
        "\n",
        "opt = optim.Adam([delta], lr=1e-1)\n",
        "\n",
        "for t in range(100):\n",
        "    pred = model(pizza_tensor + delta)\n",
        "    loss = -nn.CrossEntropyLoss()(pred, torch.LongTensor([class_names.index(\"pizza\")]).to(device)) # Don't be pizza\n",
        "    if t % 20 == 0:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    opt.zero_grad()\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    delta.data.clamp_(-epsilon, epsilon)\n",
        "\n",
        "\n",
        "print(\"After adversarial attack: Class probability:\", nn.Softmax(dim=1)(pred).detach().cpu())\n",
        "\n",
        "max_class = pred.max(dim=1)[1].item()\n",
        "print(\"After adversarial attack: Predicted class: \", class_names[max_class])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ao5ZrOhQ0VP"
      },
      "outputs": [],
      "source": [
        "pred = model(pizza_tensor + delta)\n",
        "max_class = pred.max(dim=1)[1].item()\n",
        "print(\"After adversarial attack: Predicted class: \", class_names[max_class])\n",
        "print(\"Predicted probability:\", nn.Softmax(dim=1)(pred)[0,max_class].item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wd_Kd1TTRxV8"
      },
      "outputs": [],
      "source": [
        "delta.min(), delta.max(), delta.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JoNWPgtASLCW"
      },
      "outputs": [],
      "source": [
        "plt.imshow(tensor_to_np(pizza_tensor + delta))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUuEo1OcTHVi"
      },
      "outputs": [],
      "source": [
        "delta.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CULUgzl8TKrQ"
      },
      "outputs": [],
      "source": [
        "plt.imshow(( (delta + epsilon)/epsilon * 127.5 ) [0].detach().cpu().numpy().transpose(1,2,0))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}